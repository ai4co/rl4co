{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mDPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "from omegaconf import DictConfig\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from rl4co.envs import DPPEnv, MDPPEnv\n",
        "from rl4co.data.dataset import tensordict_collate_fn, TensorDictDataset\n",
        "from rl4co.models import AttentionModel, AttentionModelPolicy\n",
        "from rl4co.tasks.rl4co import RL4COLitModule\n",
        "from rl4co.models.rl.reinforce.baselines import CriticBaseline, RolloutBaseline, WarmupBaseline, ExponentialBaseline\n",
        "from rl4co.models.rl.reinforce.critic import CriticNetwork\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mDPP Environment\n",
        "\n",
        "We declare the environment here. This will automatically download the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data_dir = \"../../data/\"\n",
        "data_file = \"mdpp/mdpp10_test_seed1234.npz\" # remember to generate data first\n",
        "\n",
        "\n",
        "# In the mDPP problem we train directly on the test data as we want to search for best config\n",
        "# of chip placement\n",
        "\n",
        "env = MDPPEnv(reward_type=\"minmax\", max_decaps=20, data_dir=data_dir,val_file=data_file, test_file=data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "bs = 10\n",
        "\n",
        "\n",
        "td = env.load_data(os.path.join(data_dir, data_file))\n",
        "td = env.reset(td)\n",
        "\n",
        "td_init = td.clone()\n",
        "\n",
        "actions = []\n",
        "probes = td['probe'].clone()\n",
        "keepouts = td['action_mask'].clone() # this includes also the probes + decaps placed later\n",
        "\n",
        "def random_policy(td):\n",
        "    \"\"\"Helper function to select a random action from available actions\"\"\"\n",
        "    action = torch.multinomial(td[\"action_mask\"].float(), 1).squeeze(-1)\n",
        "    td.set(\"action\", action)\n",
        "    return td\n",
        "\n",
        "for i in range(20):\n",
        "    # pick random action from action_mask\n",
        "    td = random_policy(td)\n",
        "    actions.append(td['action'])\n",
        "    td = env.step(td)['next']\n",
        "\n",
        "actions_ = torch.stack(actions, dim=1)\n",
        "\n",
        "# env.render(td[0], actions_[0])\n",
        "\n",
        "# for i in range(3):\n",
        "#     env.render(td[i], actions_[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action_mask: Tensor(shape=torch.Size([100, 100]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        locs: Tensor(shape=torch.Size([100, 100, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "        probe: Tensor(shape=torch.Size([100, 100]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
              "    batch_size=torch.Size([100]),\n",
              "    device=None,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "td = env.generate_data([100])\n",
        "td\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reward': tensor([ 8.0394, 11.8452,  8.5976,  8.9174,  7.8083,  8.4916,  7.5874,  7.8751,\n",
            "         7.9642,  6.9693, 11.1522,  7.6550, 10.5191,  7.6610,  7.7520,  8.4954,\n",
            "         8.4003,  3.3802, 10.1701,  7.9172,  7.7835, 10.4013, 10.3987,  7.5925,\n",
            "         8.2038, 11.6576,  8.8661,  8.0981,  7.7675,  7.4283, 10.0167,  7.5309,\n",
            "        10.6784,  7.2331,  9.5805,  7.4523,  7.3269,  8.3388,  7.4043,  8.9661,\n",
            "        10.8520,  7.6057,  8.8645,  7.5868,  8.7847,  9.3867,  8.6568,  7.4560,\n",
            "         7.7136,  8.7763,  7.3308,  8.2870,  8.5062,  8.8314,  7.9809,  9.0053,\n",
            "         7.9397,  8.5664,  7.0986,  9.0066,  7.9342,  8.3744,  7.6602,  9.5707]), 'log_likelihood': tensor([-81.3108, -43.8375, -71.3171, -75.7397, -74.5487, -72.9600, -69.3234,\n",
            "        -59.2570, -63.3589, -68.9168, -56.5742, -69.1350, -67.9623, -66.7593,\n",
            "        -70.8051, -57.2126, -50.5089, -70.1746, -51.8714, -67.3732, -68.8033,\n",
            "        -52.8647, -71.9332, -67.2134, -56.0475, -70.5145, -76.7348, -74.3200,\n",
            "        -66.5085, -70.2065, -72.1121, -64.7762, -61.5984, -70.4389, -62.4610,\n",
            "        -69.8227, -39.6748, -73.9170, -68.0881, -67.3674, -55.1593, -60.0810,\n",
            "        -69.8878, -70.1129, -74.5515, -76.5696, -70.1229, -69.5293, -68.9352,\n",
            "        -74.7691, -60.7868, -70.3738, -72.8260, -73.1666, -70.5504, -79.0380,\n",
            "        -71.7058, -67.4889, -63.0674, -65.4431, -70.5221, -73.8469, -68.3937,\n",
            "        -42.6608], grad_fn=<SumBackward1>)}\n"
          ]
        }
      ],
      "source": [
        "# Load environment with test data\n",
        "\n",
        "dataset = env.dataset(phase=\"test\")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,  # no need to shuffle, we're resampling every epoch\n",
        "    num_workers=0,\n",
        "    collate_fn=tensordict_collate_fn,\n",
        ")\n",
        "\n",
        "policy = AttentionModelPolicy(\n",
        "    env,\n",
        ")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "td = next(iter(dataloader))\n",
        "td = env.reset(td)\n",
        "\n",
        "out = policy(td, decode_type=\"greedy\")\n",
        "\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reward': tensor([ 8.2368, 11.8128,  9.2849,  6.8193, 10.4362,  8.6348,  7.9403,  8.1434,\n",
            "         8.5038,  5.1087, 10.4515,  7.9250,  8.6036,  8.1896, 10.5730,  9.0818,\n",
            "         6.8148,  4.3167,  9.1588,  8.9759,  9.4067, 10.2980, 10.2500,  7.9008,\n",
            "         8.9965,  8.3401,  9.1112,  9.5131,  7.9479,  6.8404,  9.3117,  8.1451,\n",
            "         9.1867,  6.3442,  9.2355,  9.4083,  7.6151,  8.1510,  8.6830,  9.0115,\n",
            "        10.3315,  8.5659,  7.1965,  9.0243,  8.5102,  8.0710,  8.5259,  7.5151,\n",
            "         7.5097,  9.3952,  7.9826,  8.0221,  8.2924,  9.3855,  8.7232,  7.5561,\n",
            "         8.3399,  8.5177,  7.2788,  9.3284,  7.1690,  7.7732,  9.9620,  9.5456]), 'log_likelihood': tensor([-86.5831, -57.9129, -82.1689, -81.6460, -82.9649, -82.0851, -83.9947,\n",
            "        -75.3488, -70.9338, -83.0770, -76.4443, -76.4570, -71.9629, -81.8368,\n",
            "        -82.0169, -69.0171, -65.1886, -78.4407, -72.7699, -77.2605, -79.3863,\n",
            "        -68.5387, -86.9722, -77.7829, -77.8572, -80.9965, -84.6107, -84.7199,\n",
            "        -76.3303, -82.0651, -77.6814, -75.5688, -67.4674, -82.3954, -79.1253,\n",
            "        -77.7355, -47.1026, -78.4393, -81.9808, -79.1039, -65.8121, -71.1180,\n",
            "        -78.3057, -83.0882, -80.0659, -83.4570, -78.1320, -79.9180, -84.2152,\n",
            "        -81.4511, -80.6253, -79.5312, -78.3286, -79.9796, -81.3256, -84.3026,\n",
            "        -80.5772, -77.1717, -73.3572, -76.3457, -86.0331, -83.8379, -82.2355,\n",
            "        -54.7436], grad_fn=<SumBackward1>), 'loss': tensor(-1.7292, grad_fn=<SubBackward0>), 'reinforce_loss': tensor(-1.7292, grad_fn=<NegBackward0>), 'bl_loss': 0, 'bl_val': tensor(8.5192)}\n"
          ]
        }
      ],
      "source": [
        "model = AttentionModel(\n",
        "    env,\n",
        "    policy,\n",
        ")\n",
        "\n",
        "\n",
        "td = next(iter(dataloader))\n",
        "td = env.reset(td)\n",
        "\n",
        "out = model(td, decode_type=\"sampling\")\n",
        "\n",
        "print(out)\n",
        "\n",
        "td = env.reset(td)\n",
        "init_td = td.clone()\n",
        "\n",
        "\n",
        "# out = model(td, decode_type=\"sampling\")\n",
        "\n",
        "# print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Plot\n",
        "\n",
        "probes = td['probe'].clone().cpu()\n",
        "keepouts = td['action_mask'].clone().cpu()\n",
        "\n",
        "out = policy(init_td.clone(), decode_type=\"greedy\", return_actions=True)\n",
        "decaps = out['actions'].cpu()\n",
        "\n",
        "# for i in range(3):\n",
        "#     env.render(init_td[i], decaps[i])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = DictConfig(\n",
        "    {\"data\": {\n",
        "            \"train_size\": 1000, #1000, # with 1 epochs, this is 1k samples\n",
        "            \"val_size\": 100,\n",
        "            # \"batch_size\": 64,            # \"batch_size\": 64,\n",
        "            \"batch_size\": 16,\n",
        "            \"val_batch_size\": 128,\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"lr\": 1e-4,\n",
        "        # \"lr\": 3e-5,\n",
        "        \"weight_decay\": 1e-4,\n",
        "    },\n",
        "    \"num_epochs\": 10,\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Change baseline (optional, defaults to RolloutBaseline)\n",
        "# Recreate model (so CUDA is initialized in the model)\n",
        "\n",
        "# baseline = ExponentialBaseline()\n",
        "baseline = CriticBaseline(CriticNetwork(env))\n",
        "# baseline = (WarmupBaseline(RolloutBaseline()))\n",
        "\n",
        "# Recreate model (so CUDA is initialized in the model)\n",
        "model = AttentionModel(env, policy, baseline=baseline)\n",
        "\n",
        "# model = AttentionModel(env)\n",
        "\n",
        "lit_module = RL4COLitModule(cfg=config, env=env, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "No optimizer specified, using default\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | MDPPEnv        | 0     \n",
            "1 | model | AttentionModel | 1.4 M \n",
            "-----------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.677     Total estimated model params size (MB)\n",
            "2023-06-11 22:17:03.998449: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-11 22:17:04.036030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-11 22:17:04.598778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19a3e985556e4944a1446a1a64a753bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0941ca769e1346c398023822ab3cb7aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/HDD/botu/botu/Dev/rl4co/notebooks/dpp/../../rl4co/models/rl/reinforce/baselines.py:121: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return v.detach(), -F.mse_loss(v, c.detach())\n",
            "/mnt/HDD/botu/botu/Dev/rl4co/notebooks/dpp/../../rl4co/models/rl/reinforce/baselines.py:121: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return v.detach(), -F.mse_loss(v, c.detach())\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "760fd95b762244158cfd529c144d03b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0270e2e32a464fb5a1aaf24891fa1ef1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6af7c46d7f8435a95b1f68e14259f00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65ca20bd5804b30b12dcb07797a14f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6834f10aa7e947268a5a91bf0cc51df3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13907975436e45188a9c77a8893d6c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f15978ef9dd40dea368f611b21dec78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4904a9f2845148ddb8974483f65108b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6939c7511a74cbf969acf8d17491ec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dd204bba1744372b8d566272909e6b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=config.num_epochs, # only few epochs\n",
        "    accelerator=\"gpu\", # use GPU if available, else you can use others as \"cpu\"\n",
        "    devices=[0], # GPU number, or multiple GPUs [0, 1, 2, ...]\n",
        "    logger=None, # can replace with WandbLogger, TensorBoardLogger, etc.\n",
        "    precision=\"16-mixed\", # Lightning will handle faster training with mixed precision\n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        "    reload_dataloaders_every_n_epochs=1, # necessary for sampling new data\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(lit_module)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9.6373, device='cuda:0')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lit_module.test_batch_size = 100 # so we load the whole test set\n",
        "dl = lit_module.test_dataloader()\n",
        "policy = lit_module.model.policy.to(\"cuda\")\n",
        "batch = next(iter(dl))\n",
        "td = env.reset(batch).to(\"cuda\")\n",
        "out = policy(td, decode_type=\"greedy\")\n",
        "out['reward'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out['reward'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: tensor(7.5768)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m33\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m17\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReward:\u001b[39m\u001b[39m\"\u001b[39m, rewards[i])\n\u001b[0;32m---> 13\u001b[0m     env\u001b[39m.\u001b[39mrender(init_td[i], actions[i])\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "## Plot\n",
        "\n",
        "init_td = env.reset(batch).to(\"cuda\")\n",
        "probes = init_td['probe'].clone().cpu()\n",
        "keepouts = init_td['action_mask'].clone().cpu()\n",
        "\n",
        "out = policy(init_td.clone(), decode_type=\"greedy\", return_actions=True)\n",
        "decaps = out['actions'].cpu()\n",
        "rewards = out['reward'].cpu()\n",
        "\n",
        "for i in [33, 50, 17]:\n",
        "    print(\"Reward:\", rewards[i])\n",
        "    env.render(init_td[i], actions[i])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
