{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POMO Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from ncobench.envs.tsp import TSPEnv\n",
        "from ncobench.models.rl.reinforce import *\n",
        "from ncobench.models.co.am.context import env_context\n",
        "from ncobench.models.co.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from ncobench.models.co.am.encoder import GraphAttentionEncoder\n",
        "from ncobench.models.co.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from ncobench.models.co.am.policy import get_log_likelihood\n",
        "from ncobench.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from ncobench.utils.lightning import get_lightning_device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Novelty compared to `AttentionModel`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pseudo-code of differences for training\n",
        "\n",
        "### Attention Model\n",
        "```python\n",
        "def train(policy network p_Î¸, training set S, batch size B, significance Î±):\n",
        "    for i in 1...B:\n",
        "        s_i = RandomInstance(S)\n",
        "        Ï€_i = SampleRollout(s_i, p_Î¸)\n",
        "        vb = UpdateBaseline(s, Ï€)\n",
        "        âˆ‡ğ¿ = (1/B) * Î£(L(Ï€_i|s_i) - b_i) * âˆ‡_Î¸ log(p_Î¸(Ï€_i|s_i))\n",
        "        Î¸ = GradientDescent(Î¸, âˆ‡ğ¿)\n",
        "        if OneSidedPairedTest(p_Î¸, p_Î¸^BL) < Î±: # p_Î¸ is better than p_Î¸^BL\n",
        "            Î¸^BL = Î¸\n",
        "```\n",
        "\n",
        "### POMO\n",
        "```python\n",
        "def train(policy network p_Î¸, training set S, batch size B, number of start nodes N):\n",
        "    for i in 1...B:\n",
        "        s_i = RandomInstance(S)\n",
        "        # New: select starting nodes, and rolout with them\n",
        "        Î±_i1,...,Î±_iN = SelectStartNodes(s_i)\n",
        "        Ï€_i1,...,Ï€_iN = SampleRollout(s_i, p_Î¸, {Î±_i,j})\n",
        "        vb = UpdateBaseline(s, Ï€)\n",
        "        # New: baseline is simply the average baseline\n",
        "        âˆ‡ğ¿ = (1/NB) * Î£(L(Ï€_ij|s_i) - b_i) * âˆ‡_Î¸ log(p_Î¸(Ï€_ij|s_i))\n",
        "        Î¸ = GradientDescent(Î¸, âˆ‡ğ¿)\n",
        "```\n",
        "\n",
        "So the novelty is:\n",
        "1. We select a set of starting nodes for each instance and rollout with them\n",
        "2. Replace baseline with average baseline\n",
        "\n",
        "---\n",
        "\n",
        "## Other novelty\n",
        "1. Use `multi-greedy` decoding (e.g.) simply take the best out of the starting points\n",
        "2. Use Instance Augmentation (e.g. just augment the dataset with)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f28cd7d70d0>>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For easier debugging\n",
        "\n",
        "from rich.traceback import install\n",
        "install()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilities: action selection, batching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @torch.compile\n",
        "def select_start_nodes(batch_size, num_nodes, device=\"cpu\"):\n",
        "    \"\"\"Node selection strategy for POMO\n",
        "    Selects different start nodes for each batch element\n",
        "    \"\"\"\n",
        "    selected = torch.arange(num_nodes, device=device).repeat_interleave(batch_size, dim=0) # TODO: check\n",
        "    # requires grad\n",
        "    selected.requires_grad_ = True # TODO check\n",
        "    return selected\n",
        "\n",
        "\n",
        "# @torch.compile\n",
        "def repeat_batch(x, repeats):\n",
        "    \"\"\"Same as repeat_interleave on dim=0 for tensordicts as well\n",
        "    Same as einops.repeat(x, 'b n d -> (r b) n d', r=repeats) but 50% faster\n",
        "    \"\"\"\n",
        "    s = x.shape\n",
        "    return x.expand(repeats, *s).contiguous().view(s[0] * repeats, *s[1:])\n",
        "\n",
        "\n",
        "# @torch.compile\n",
        "def undo_repeat_batch(x, repeats):\n",
        "    \"\"\"Undoes repeat_interleave_batch\n",
        "    Same as einops.rearrange(x, '(r b) ... -> r b ...', r=repeats) but 3x faster\n",
        "    \"\"\"\n",
        "    s = x.shape\n",
        "    return x.view(repeats, s[0] // repeats, *s[1:]) # note that repeat is the first dimension!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some Quick testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([640, 32, 128])\n",
            "torch.Size([640, 32, 128])\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "from tensordict import TensorDict\n",
        "\n",
        "td = TensorDict({'a': torch.rand(64, 32, 128), 'b': torch.rand(64, 32, 128)}, batch_size=64)\n",
        "\n",
        "\n",
        "# a = torch.rand(64, 32, 128)\n",
        "# a = torch.repeat_interleave(a, 8, dim=0)\n",
        "# Same as repeat_interleave\n",
        "td = torch.rand(64, 32, 128)\n",
        "\n",
        "bs = td.shape[0]\n",
        "td_repeat = td.expand(10, *td.shape).contiguous() #.view(bs*10, *td.shape[1:])\n",
        "td_repeat = td_repeat.view(bs*10, *td.shape[1:])\n",
        "print(td_repeat.shape)\n",
        "\n",
        "a = torch.rand(64, 32, 128)\n",
        "b = a.repeat_interleave(10, dim=0)\n",
        "\n",
        "print(b.shape)\n",
        "\n",
        "\n",
        "# Test for repeat_interleave_batch\n",
        "c = repeat_batch(a, 10)\n",
        "print(torch.allclose(c, b)) # note that repeat_interleave is not the same as our implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "td = TensorDict({\n",
        "    \"observation\": torch.randn(5, 10, 2),\n",
        "},batch_size=5)\n",
        "\n",
        "td_ = repeat_batch(td, 3)\n",
        "\n",
        "td_ = undo_repeat_batch(td_, 3)\n",
        "print(td_.shape)\n",
        "\n",
        "torch.allclose(td['observation'], td_['observation'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PrecomputedCache:\n",
        "    node_embeddings: torch.Tensor\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, env, embedding_dim, num_heads, num_pomo=20, **logit_attn_kwargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = num_heads\n",
        "\n",
        "        assert embedding_dim % num_heads == 0\n",
        "\n",
        "        step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n",
        "        self.context = env_context(self.env.name, {\"context_dim\": step_context_dim})\n",
        "        self.dynamic_embedding = env_dynamic_embedding(\n",
        "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
        "        )\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(\n",
        "            embedding_dim, 3 * embedding_dim, bias=False\n",
        "        )\n",
        "\n",
        "        # TODO: check: POMO has projection for first node, but not for fixed context\n",
        "        # self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_first_node_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_step_context = nn.Linear(\n",
        "            step_context_dim, embedding_dim, bias=False\n",
        "        )\n",
        "\n",
        "        # MHA\n",
        "        self.logit_attention = LogitAttention(\n",
        "            embedding_dim, num_heads, **logit_attn_kwargs\n",
        "        )\n",
        "\n",
        "        # POMO\n",
        "        self.num_pomo = max(num_pomo, 1) # POMO = 1 is just normal REINFORCE\n",
        "\n",
        "    def forward(self, td, embeddings, decode_type=\"sampling\"):\n",
        "        # Collect outputs\n",
        "        outputs = []\n",
        "        actions = []\n",
        "\n",
        "        if self.num_pomo > 1:\n",
        "            # POMO: first action is decided via select_start_nodes\n",
        "            action = select_start_nodes(batch_size=td.shape[0], num_nodes=self.num_pomo, device=td.device)\n",
        "\n",
        "            # # Expand td to batch_size * num_pomo\n",
        "            td = repeat_batch(td, self.num_pomo)\n",
        "\n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "            log_p = torch.zeros_like(td['action_mask'], device=td.device) # first log_p is 0, so p = log_p.exp() = 1\n",
        "\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "        \n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        cached_embeds = self._precompute(embeddings)        \n",
        "\n",
        "        # Here we suppose all the batch is done at the same time\n",
        "        while not td[\"done\"].any():  \n",
        "            # Compute the logits for the next node\n",
        "            log_p, mask = self._get_log_p(cached_embeds, td)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            action = decode_probs(\n",
        "                log_p.exp().squeeze(1), mask.squeeze(1), decode_type=decode_type\n",
        "            )\n",
        "\n",
        "            # Step the environment\n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "\n",
        "        outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
        "        td.set(\"reward\", self.env.get_reward(td[\"observation\"], actions))\n",
        "        return outputs, actions, td\n",
        "    \n",
        "    def _precompute(self, embeddings):       \n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        (\n",
        "            glimpse_key_fixed,\n",
        "            glimpse_val_fixed,\n",
        "            logit_key_fixed,\n",
        "        ) = self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "\n",
        "        # Organize in a dataclass for easy access\n",
        "        cached_embeds = PrecomputedCache(\n",
        "            node_embeddings=repeat_batch(embeddings, self.num_pomo),\n",
        "            glimpse_key=repeat_batch(self.logit_attention._make_heads(glimpse_key_fixed), self.num_pomo),\n",
        "            glimpse_val=repeat_batch(self.logit_attention._make_heads(glimpse_val_fixed), self.num_pomo),\n",
        "            logit_key=repeat_batch(logit_key_fixed, self.num_pomo)\n",
        "        )\n",
        "\n",
        "        return cached_embeds\n",
        "\n",
        "    def _get_log_p(self, cached, td):\n",
        "        # Compute the query based on the context (computes automatically the first and last node context)\n",
        "        context = self.context(cached.node_embeddings, td)\n",
        "        step_context = self.project_step_context(context)  # [batch, 1, embed_dim]\n",
        "        query = step_context # in POMO, no graph context (trick for overfit) # [batch, 1, embed_dim]\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        glimpse_key_dynamic, glimpse_val_dynamic, logit_key_dynamic = self.dynamic_embedding(td)\n",
        "        glimpse_key = cached.glimpse_key + glimpse_key_dynamic\n",
        "        glimpse_key = cached.glimpse_val + glimpse_val_dynamic\n",
        "        logit_key = cached.logit_key + logit_key_dynamic\n",
        "\n",
        "        # Get the mask\n",
        "        mask = ~td[\"action_mask\"]\n",
        "\n",
        "        # Compute logits\n",
        "        log_p = self.logit_attention(query, glimpse_key, glimpse_key, logit_key, mask)\n",
        "\n",
        "        return log_p, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POMOPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: EnvBase,\n",
        "                 embedding_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 encoder: nn.Module = None,\n",
        "                 decoder: nn.Module = None,\n",
        "                 num_pomo: int = 10,\n",
        "                 n_encode_layers: int = 3,\n",
        "                 normalization: str = 'batch',\n",
        "                 n_heads: int = 8,\n",
        "                 checkpoint_encoder: bool = False,\n",
        "                 mask_inner: bool = True,\n",
        "                 force_flash_attn: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(POMOPolicy, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.env = env\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.checkpoint_encoder = checkpoint_encoder\n",
        "\n",
        "        self.init_embedding = env_init_embedding(self.env.name, {\"embedding_dim\": embedding_dim})\n",
        "\n",
        "        self.encoder = GraphAttentionEncoder(\n",
        "            n_heads=n_heads,\n",
        "            embed_dim=embedding_dim,\n",
        "            n_layers=self.n_encode_layers,\n",
        "            normalization=normalization,\n",
        "            force_flash_attn=force_flash_attn,\n",
        "        ) if encoder is None else encoder\n",
        "        \n",
        "        self.decoder = Decoder(env, embedding_dim, n_heads, num_pomo=num_pomo, mask_inner=mask_inner, force_flash_attn=force_flash_attn) if decoder is None else decoder\n",
        "        self.num_pomo = num_pomo\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str = \"train\", decode_type: str = \"sampling\") -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        embedding = self.init_embedding(td)\n",
        "        encoded_inputs, _ = self.encoder(embedding)\n",
        "\n",
        "        # Main rollout\n",
        "        _log_p, actions, td = self.decoder(td, encoded_inputs, decode_type)\n",
        "\n",
        "        # Max POMO reward\n",
        "        reward_ = undo_repeat_batch(td[\"reward\"], self.num_pomo)\n",
        "        max_reward, max_idxs = reward_.max(dim=0) # TODO: check if this is correct\n",
        "        # print(reward_.shape)\n",
        "        actions_ = undo_repeat_batch(actions, self.num_pomo)\n",
        "        best_actions = actions_.gather(0, max_idxs[:, None, None])\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        ll = get_log_likelihood(_log_p, actions, td.get('mask', None))\n",
        "        out = {\"reward\": td[\"reward\"], \"log_likelihood\": ll, \"actions\": actions, \"max_reward\": max_reward, \"best_actions\": best_actions}\n",
        "\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Policy only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 â”‚   â”‚   â”‚   â”‚   </span>collate_fn=torch.stack, <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># we need this to stack the batches in the datas</span>    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 â”‚   â”‚   â”‚   </span>)                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>14 model = POMOPolicy(                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 â”‚   </span>env,                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 â”‚   </span>embedding_dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span>,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 â”‚   </span>hidden_dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span>,                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">38</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 â”‚   â”‚   â”‚   </span>force_flash_attn=force_flash_attn,                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 â”‚   â”‚   </span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> encoder <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> encoder                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 â”‚   â”‚   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>38 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder = Decoder(env, embedding_dim, n_heads, num_pomo=num_pomo, mask_inne    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_pomo = num_pomo                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 â”‚   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, td: TensorDict, phase: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = <span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>, decode_type: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = <span style=\"color: #808000; text-decoration-color: #808000\">\"sampling</span>    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> embedding_dim % num_heads == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 â”‚   â”‚   </span>step_context_dim = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> * embedding_dim  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Embedding of first and last node</span>           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.context = env_context(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.env.name, {<span style=\"color: #808000; text-decoration-color: #808000\">\"context_dim\"</span>: step_context_dim})       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dynamic_embedding = env_dynamic_embedding(                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.env.name, {<span style=\"color: #808000; text-decoration-color: #808000\">\"embedding_dim\"</span>: embedding_dim}                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 â”‚   â”‚   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/botu/Dev/ncobench/notebooks/pomo/../../ncobench/models/co/am/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">context.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">env_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> context_class <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Unknown environment name '{</span>env_name<span style=\"color: #808000; text-decoration-color: #808000\">}'\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> context_class(**config)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">EnvContext</span>(nn.Module):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TSPContext.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'context_dim'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m14\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mcollate_fn=torch.stack, \u001b[2m# we need this to stack the batches in the datas\u001b[0m    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m)                                                                               \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m13 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m14 model = POMOPolicy(                                                                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2mâ”‚   \u001b[0menv,                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2mâ”‚   \u001b[0membedding_dim=\u001b[94m128\u001b[0m,                                                                      \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2mâ”‚   \u001b[0mhidden_dim=\u001b[94m128\u001b[0m,                                                                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m38\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mforce_flash_attn=force_flash_attn,                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m) \u001b[94mif\u001b[0m encoder \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m encoder                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m38 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.decoder = Decoder(env, embedding_dim, n_heads, num_pomo=num_pomo, mask_inne    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.num_pomo = num_pomo                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                        \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, td: TensorDict, phase: \u001b[96mstr\u001b[0m = \u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m, decode_type: \u001b[96mstr\u001b[0m = \u001b[33m\"\u001b[0m\u001b[33msampling\u001b[0m    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94massert\u001b[0m embedding_dim % num_heads == \u001b[94m0\u001b[0m                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mstep_context_dim = \u001b[94m2\u001b[0m * embedding_dim  \u001b[2m# Embedding of first and last node\u001b[0m           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 23 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.context = env_context(\u001b[96mself\u001b[0m.env.name, {\u001b[33m\"\u001b[0m\u001b[33mcontext_dim\u001b[0m\u001b[33m\"\u001b[0m: step_context_dim})       \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.dynamic_embedding = env_dynamic_embedding(                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.env.name, {\u001b[33m\"\u001b[0m\u001b[33membedding_dim\u001b[0m\u001b[33m\"\u001b[0m: embedding_dim}                                \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m)                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/home/botu/Dev/ncobench/notebooks/pomo/../../ncobench/models/co/am/\u001b[0m\u001b[1;33mcontext.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92menv_context\u001b[0m  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mif\u001b[0m context_class \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnknown environment name \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0menv_name\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 23 \u001b[2mâ”‚   \u001b[0m\u001b[94mreturn\u001b[0m context_class(**config)                                                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 24 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 25 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mEnvContext\u001b[0m(nn.Module):                                                               \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTSPContext.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'context_dim'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_loc = 15\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "dataset = env.dataset(batch_size=[10000])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=32,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = POMOPolicy(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    num_pomo=num_loc,\n",
        "    # force_flash_attn=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "x = env.reset(init_obs=x)\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create full model: `env` + `policy` + `baseline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POMO(nn.Module):\n",
        "    def __init__(self, env, policy, baseline):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.baseline = baseline\n",
        "\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "\n",
        "        costs = undo_repeat_batch(-out_policy['reward'], self.policy.num_pomo)\n",
        "        ll = undo_repeat_batch(out_policy['log_likelihood'], self.policy.num_pomo)\n",
        "        \n",
        "        bl_val, bl_loss = self.baseline.eval(td, costs)\n",
        "\n",
        "        # Calculate loss\n",
        "        advantage = costs - bl_val\n",
        "        reinforce_loss = (advantage * ll).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "\n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        if hasattr(self.baseline, \"setup\"):\n",
        "            self.baseline.setup(self.policy, lit_module.train_dataloader(), self.env, device=get_lightning_device(lit_module))\n",
        "    \n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.env.dataset(self.train_size)\n",
        "        self.val_dataset = self.env.dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_obs=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        self.log(f\"{phase}/pomo_cost\", -output[\"max_reward\"].mean(), prog_bar=True)\n",
        "        \n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-6)\n",
        "        # optim = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.env.dataset(self.train_size) \n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>env = TSPEnv(num_loc=num_loc).transform()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 # Policy</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>14 policy = POMOPolicy(                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 â”‚   </span>env,                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 â”‚   </span>num_pomo=num_pomo,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 â”‚   </span>embedding_dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span>,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">38</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 â”‚   â”‚   â”‚   </span>force_flash_attn=force_flash_attn,                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 â”‚   â”‚   </span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> encoder <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> encoder                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 â”‚   â”‚   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>38 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder = Decoder(env, embedding_dim, n_heads, num_pomo=num_pomo, mask_inne    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_pomo = num_pomo                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 â”‚   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, td: TensorDict, phase: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = <span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>, decode_type: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = <span style=\"color: #808000; text-decoration-color: #808000\">\"sampling</span>    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> embedding_dim % num_heads == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 â”‚   â”‚   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 â”‚   â”‚   </span>step_context_dim = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> * embedding_dim  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Embedding of first and last node</span>           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.context = env_context(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.env.name, {<span style=\"color: #808000; text-decoration-color: #808000\">\"context_dim\"</span>: step_context_dim})       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dynamic_embedding = env_dynamic_embedding(                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.env.name, {<span style=\"color: #808000; text-decoration-color: #808000\">\"embedding_dim\"</span>: embedding_dim}                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 â”‚   â”‚   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/botu/Dev/ncobench/notebooks/pomo/../../ncobench/models/co/am/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">context.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">env_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> context_class <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Unknown environment name '{</span>env_name<span style=\"color: #808000; text-decoration-color: #808000\">}'\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> context_class(**config)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">EnvContext</span>(nn.Module):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TSPContext.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'context_dim'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m14\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m11 \u001b[0menv = TSPEnv(num_loc=num_loc).transform()                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m# Policy\u001b[0m                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m14 policy = POMOPolicy(                                                                        \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2mâ”‚   \u001b[0menv,                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2mâ”‚   \u001b[0mnum_pomo=num_pomo,                                                                      \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2mâ”‚   \u001b[0membedding_dim=\u001b[94m128\u001b[0m,                                                                      \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m38\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mforce_flash_attn=force_flash_attn,                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m) \u001b[94mif\u001b[0m encoder \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m encoder                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m38 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.decoder = Decoder(env, embedding_dim, n_heads, num_pomo=num_pomo, mask_inne    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.num_pomo = num_pomo                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                        \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, td: TensorDict, phase: \u001b[96mstr\u001b[0m = \u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m, decode_type: \u001b[96mstr\u001b[0m = \u001b[33m\"\u001b[0m\u001b[33msampling\u001b[0m    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94massert\u001b[0m embedding_dim % num_heads == \u001b[94m0\u001b[0m                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mstep_context_dim = \u001b[94m2\u001b[0m * embedding_dim  \u001b[2m# Embedding of first and last node\u001b[0m           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 23 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.context = env_context(\u001b[96mself\u001b[0m.env.name, {\u001b[33m\"\u001b[0m\u001b[33mcontext_dim\u001b[0m\u001b[33m\"\u001b[0m: step_context_dim})       \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.dynamic_embedding = env_dynamic_embedding(                                    \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.env.name, {\u001b[33m\"\u001b[0m\u001b[33membedding_dim\u001b[0m\u001b[33m\"\u001b[0m: embedding_dim}                                \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m)                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/home/botu/Dev/ncobench/notebooks/pomo/../../ncobench/models/co/am/\u001b[0m\u001b[1;33mcontext.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92menv_context\u001b[0m  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mif\u001b[0m context_class \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                              \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnknown environment name \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0menv_name\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 23 \u001b[2mâ”‚   \u001b[0m\u001b[94mreturn\u001b[0m context_class(**config)                                                         \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 24 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 25 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mEnvContext\u001b[0m(nn.Module):                                                               \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTSPContext.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'context_dim'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "epochs = 1\n",
        "batch_size = 64 #1024 #512\n",
        "num_loc = 20\n",
        "train_size = 1280000\n",
        "lr = 1e-4\n",
        "num_pomo = num_loc # TODO: comment to try out = 1\n",
        "# num_pomo = 1 # set to 1: similar to simple AM\n",
        "\n",
        "# Environment\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "# Policy\n",
        "policy = POMOPolicy(\n",
        "    env,\n",
        "    num_pomo=num_pomo,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # force_flash_attn=True,\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "# baseline = WarmupBaseline(RolloutBaseline())\n",
        "baseline = SharedBaseline() # TODO: uncomment\n",
        "\n",
        "# Create RL model\n",
        "model = POMO(env, policy, baseline)\n",
        "\n",
        "# Create Lightning module (for training)\n",
        "lit_model = NCOLightningModule(env, model, batch_size=batch_size, train_size=train_size, lr=lr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 # Fit the model</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>16 trainer.fit(lit_model)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'lit_model'</span> is not defined\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m13 \u001b[0m)                                                                                           \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m14 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m# Fit the model\u001b[0m                                                                             \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m16 trainer.fit(lit_model)                                                                      \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ”‚\u001b[0m   \u001b[2m17 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
              "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'lit_model'\u001b[0m is not defined\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    logger=None, # can replace with WandbLogger, TensorBoardLogger, etc.\n",
        "    # precision=16, # uncomment to make faster\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients!\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(lit_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
