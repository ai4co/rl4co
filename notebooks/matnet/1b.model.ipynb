{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MatNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(2*\"../\")\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchrl.modules.models.models import MLP\n",
        "from torchrl.envs import EnvBase\n",
        "from tensordict.tensordict import TensorDict\n",
        "\n",
        "from rl4co.envs import ATSPEnv \n",
        "from rl4co.utils.ops import batchify, unbatchify, select_start_nodes\n",
        "from rl4co.models.nn.attention import LogitAttention\n",
        "from rl4co.models.nn.utils import decode_probs, get_log_likelihood\n",
        "from rl4co.models.nn.env_context import env_context\n",
        "from rl4co.models.nn.env_embedding import env_dynamic_embedding, env_init_embedding\n",
        "from rl4co.data.dataset import tensordict_collate_fn\n",
        "from rl4co.tasks.rl4co import RL4COLitModule"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differences between AM and MatNet\n",
        "\n",
        "1. MatNet uses a dual graph attention layer for processing the  set of source and destination nodes A and B separately\n",
        "2. Mixed-score attention: this should make the network learn the \"best\" recipe\n",
        "3. Initial node representation: zero-vectors for A nodes and one-hot vectors for B nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action_mask: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        cost_matrix: Tensor(shape=torch.Size([10, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "        current_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        first_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        i: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ATSPEnv(num_loc=10)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test out\n",
        "\n",
        "# col_emb.shape: (batch, col_cnt, embedding)\n",
        "# row_emb.shape: (batch, row_cnt, embedding)\n",
        "# cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "batch = 64\n",
        "row_cnt = 20\n",
        "col_cnt = 30\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    'embedding_dim': 256,\n",
        "    'sqrt_embedding_dim': 256**(1/2),\n",
        "    'encoder_layer_num': 5,\n",
        "    'qkv_dim': 16,\n",
        "    'sqrt_qkv_dim': 16**(1/2),\n",
        "    'head_num': 16,\n",
        "    'logit_clipping': 10,\n",
        "    'ff_hidden_dim': 512,\n",
        "    'ms_hidden_dim': 16,\n",
        "    'ms_layer1_init': (1/2)**(1/2),\n",
        "    'ms_layer2_init': (1/16)**(1/2),\n",
        "    'eval_type': 'argmax',\n",
        "    'one_hot_seed_cnt': 20,  # must be >= node_cnt\n",
        "}\n",
        "\n",
        "\n",
        "row_emb = torch.randn(batch, row_cnt, model_params['embedding_dim'])\n",
        "col_emb = torch.randn(batch, col_cnt, model_params['embedding_dim'])\n",
        "cost_mat = torch.randn(batch, row_cnt, col_cnt)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ours (all we need)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MixedScoreMHA(nn.Module):\n",
        "    def __init__(self, \n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    hidden_dim: int = 16,\n",
        "                    qkv_dim: int = 16,\n",
        "                    bias=False,\n",
        "                    layer1_init: float = (1/2)**(1/2),\n",
        "                    layer2_init: float = (1/16)**(1/2),\n",
        "                    device=None,\n",
        "                    dtype=None\n",
        "        ):\n",
        "        super().__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        assert (embed_dim % num_heads == 0), \"embed_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Project\n",
        "        self.Wq = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.Wk = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.Wv = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.out_proj = nn.Linear(num_heads*qkv_dim, embed_dim, **factory_kwargs)\n",
        "\n",
        "        # Init mix params\n",
        "        self.mix1_weight = nn.Parameter(torch.empty(num_heads, 2, hidden_dim).uniform_(-layer1_init, layer1_init))\n",
        "        self.mix1_bias = nn.Parameter(torch.empty(num_heads, hidden_dim).uniform_(-layer1_init, layer1_init))\n",
        "        self.mix2_weight = nn.Parameter(torch.empty(num_heads, hidden_dim, 1).uniform_(-layer2_init, layer2_init))\n",
        "        self.mix2_bias = nn.Parameter(torch.empty(num_heads, 1).uniform_(-layer2_init, layer2_init))\n",
        "\n",
        "    def forward(self, q, k, v, matrix):\n",
        "        # Project q, k, v and reshape to [batch, head_num, row_cnt, hidden_dim]\n",
        "        # q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
        "        # q, k, v = map(lambda t: self._reshape_heads(t), (q, k, v))\n",
        "        q = self._make_heads(self.Wq(q))\n",
        "        k = self._make_heads(self.Wk(k))\n",
        "        v = self._make_heads(self.Wv(v))\n",
        "\n",
        "        # Prepare dot product and matrix score: [batch, head_num, row_cnt, col_cnt]\n",
        "        dot_product = torch.einsum('...rd,...cd->...rc', q, k) / math.sqrt(q.shape[-1])\n",
        "        matrix_score = repeat(matrix, 'b r c -> b h r c', h=self.num_heads)\n",
        "\n",
        "        # Mix the scores. Use einsum for best performance\n",
        "        two_scores = torch.stack((dot_product, matrix_score), dim=-1)\n",
        "        ms1 = torch.einsum('bhrct,htd->brhcd', two_scores, self.mix1_weight)\n",
        "        ms2 = torch.einsum('brhcd,hdt->brhct', F.relu(ms1), self.mix2_weight)\n",
        "        mixed_scores = rearrange(ms2, 'b h r c 1 -> b r h c')\n",
        "\n",
        "        # Softmax and multiply with values\n",
        "        weights = F.softmax(mixed_scores, dim=3)\n",
        "        out = torch.matmul(weights, v)\n",
        "        \n",
        "        # Project out\n",
        "        out = rearrange(out, 'b h r d -> b r (h d)')\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    def _make_heads(self, x):\n",
        "        return rearrange(x, \"b g (h s) -> b h g s\", h=self.num_heads)\n",
        "         \n",
        "    \n",
        "class AddAndInstanceNormalization(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # [batch, problem, embedding]\n",
        "        added = input1 + input2\n",
        "        normalized = self.norm(added.transpose(1, 2)).transpose(1, 2)\n",
        "        return normalized\n",
        "    \n",
        "class EncodingBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    ms_hidden_dim=16,\n",
        "                    ff_hidden_dim=512,\n",
        "                    **mha_kwargs\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.mixed_score_mha = MixedScoreMHA(embed_dim, num_heads, ms_hidden_dim, **mha_kwargs)\n",
        "        self.add_n_normalization_1 = AddAndInstanceNormalization(embed_dim)\n",
        "        self.feed_forward = MLP(embed_dim, embed_dim, 1, ff_hidden_dim, activation_class=nn.ReLU)\n",
        "        self.add_n_normalization_2 = AddAndInstanceNormalization(embed_dim)\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        q, k, v = row_emb, col_emb, col_emb\n",
        "        out_mha = self.mixed_score_mha(q, k, v, cost_mat)\n",
        "        out1 = self.add_n_normalization_1(row_emb, out_mha)\n",
        "        out2 = self.feed_forward(out1)\n",
        "        out3 = self.add_n_normalization_2(out1, out2)\n",
        "        return out3 # shape: (batch, row_cnt, embedding)\n",
        "    \n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, **kw):\n",
        "        super().__init__()\n",
        "        self.row_encoding_block = EncodingBlock(**kw)\n",
        "        self.col_encoding_block = EncodingBlock(**kw)\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        # row_emb.shape: (batch, row_cnt, embedding)\n",
        "        # col_emb.shape: (batch, col_cnt, embedding)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "        row_emb_out = self.row_encoding_block(row_emb, col_emb, cost_mat)\n",
        "        col_emb_out = self.col_encoding_block(col_emb, row_emb, cost_mat.transpose(1, 2))\n",
        "        return row_emb_out, col_emb_out\n",
        "        \n",
        "\n",
        "class MatNetEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **kw):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(**kw) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        for layer in self.layers:\n",
        "            row_emb, col_emb = layer(row_emb, col_emb, cost_mat)\n",
        "        return row_emb, col_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 5.27 MB\n",
            "torch.Size([64, 20, 256]) torch.Size([64, 30, 256])\n"
          ]
        }
      ],
      "source": [
        "# encoder = MatNetEncoder(**model_params)\n",
        "\n",
        "encoder = MatNetEncoder(num_layers=5, embed_dim=256, num_heads=16)\n",
        "print('Number of parameters: {:.2f} MB'.format(sum(p.numel() for p in encoder.parameters() if p.requires_grad) / 1e6))\n",
        "out = encoder(row_emb, col_emb, cost_mat)\n",
        "print(out[0].shape, out[1].shape)\n",
        "#print number of parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PrecomputedCache:\n",
        "    row_embeddings: torch.Tensor\n",
        "    # graph_context: torch.Tensor # TODO: check if used in MatNet\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, env, embedding_dim, num_heads, num_starts=20, **logit_attn_kwargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        assert embedding_dim % num_heads == 0\n",
        "\n",
        "        self.context = env_context(self.env.name, {\"embedding_dim\": embedding_dim})\n",
        "        self.dynamic_embedding = env_dynamic_embedding(\n",
        "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
        "        )\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value ) so 2 * embedding_dim\n",
        "        # In original implementation, this is separated but this is the same\n",
        "        # Note that compared to original AM, we do not project the logit key\n",
        "        self.project_node_embeddings = nn.Linear(\n",
        "            embedding_dim, 2 * embedding_dim, bias=False\n",
        "        )\n",
        "    \n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "        # MHA\n",
        "        self.logit_attention = LogitAttention(\n",
        "            embedding_dim, num_heads, **logit_attn_kwargs\n",
        "        )\n",
        "\n",
        "        # POMO\n",
        "        self.num_starts = max(num_starts, 1) # POMO = 1 is just normal REINFORCE\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        td,\n",
        "        embeddings,\n",
        "        decode_type=\"sampling\",\n",
        "        softmax_temp=None,\n",
        "        single_traj=False,\n",
        "        num_starts=None,\n",
        "    ):\n",
        "\n",
        "        # Greedy multi-start decoding if num_starts > 1\n",
        "        num_starts = (\n",
        "            self.num_starts if num_starts is None else num_starts\n",
        "        )  # substitute self.num_starts with num_starts\n",
        "        assert not (\n",
        "            \"multistart\" in decode_type and num_starts <= 1\n",
        "        ), \"Multi-start decoding requires `num_starts` > 1\"\n",
        "\n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        cached_embeds = self._precompute(embeddings)\n",
        "\n",
        "        # Collect outputs\n",
        "        outputs = []\n",
        "        actions = []\n",
        "\n",
        "        if num_starts > 1 and not single_traj or \"multistart\" in decode_type:\n",
        "            # POMO: first action is decided via select_start_nodes\n",
        "            action = select_start_nodes(td, num_starts, self.env)\n",
        "\n",
        "            # Expand td to batch_size * num_starts\n",
        "            td = batchify(td, num_starts)\n",
        "\n",
        "            td.set(\"action\", action)\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "            log_p = torch.zeros_like(\n",
        "                td[\"action_mask\"], device=td.device\n",
        "            )  # first log_p is 0, so p = log_p.exp() = 1\n",
        "\n",
        "            outputs.append(log_p)\n",
        "            actions.append(action)\n",
        "\n",
        "        # Main decoding\n",
        "        while not td[\"done\"].all():\n",
        "\n",
        "            log_p, mask = self._get_log_p(cached_embeds, td, softmax_temp, num_starts)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            action = decode_probs(log_p.exp(), mask, decode_type=decode_type)\n",
        "\n",
        "            td.set(\"action\", action)\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p)\n",
        "            actions.append(action)\n",
        "\n",
        "        outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
        "        td.set(\"reward\", self.env.get_reward(td, actions))\n",
        "        return outputs, actions, td\n",
        "    \n",
        "    def _precompute(self, embeddings):\n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        row_embed, col_embed = embeddings\n",
        "    \n",
        "        (\n",
        "            glimpse_key_fixed,\n",
        "            glimpse_val_fixed,\n",
        "        ) = self.project_node_embeddings(col_embed).chunk(2, dim=-1)\n",
        "        \n",
        "        # Organize in a dataclass for easy access\n",
        "        cached_embeds = PrecomputedCache(\n",
        "            row_embeddings=row_embed,\n",
        "            glimpse_key=glimpse_key_fixed,\n",
        "            glimpse_val=glimpse_val_fixed,\n",
        "            logit_key=col_embed,\n",
        "        )\n",
        "\n",
        "        return cached_embeds\n",
        "    \n",
        "\n",
        "    def _get_log_p(self, cached, td, softmax_temp=None, num_starts=0):\n",
        "        # Compute the query based on the context (computes automatically the first and last node context)\n",
        "\n",
        "        # Unbatchify to [batch_size, num_starts, ...]. Has no effect if num_starts = 0\n",
        "        td_unbatch = unbatchify(td, num_starts)\n",
        "\n",
        "        step_context = self.context(cached.row_embeddings, td_unbatch)\n",
        "        glimpse_q = step_context  # in POMO, no graph context is used to compute query\n",
        "        glimpse_q = glimpse_q.unsqueeze(1) if glimpse_q.ndim == 2 else glimpse_q\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        (\n",
        "            glimpse_key_dynamic,\n",
        "            glimpse_val_dynamic,\n",
        "            logit_key_dynamic,\n",
        "        ) = self.dynamic_embedding(td_unbatch)\n",
        "        glimpse_k = cached.glimpse_key + glimpse_key_dynamic\n",
        "        glimpse_v = cached.glimpse_val + glimpse_val_dynamic\n",
        "        logit_k = cached.logit_key + logit_key_dynamic\n",
        "\n",
        "        # Get the mask\n",
        "        mask = ~td_unbatch[\"action_mask\"]\n",
        "\n",
        "        # Compute logits\n",
        "        log_p = self.logit_attention(\n",
        "            glimpse_q, glimpse_k, glimpse_v, logit_k, mask, softmax_temp\n",
        "        )\n",
        "\n",
        "        # Now we need to reshape the logits and log_p to [batch_size*num_starts, num_nodes]\n",
        "        # Note that rearranging order is important here\n",
        "        log_p = rearrange(log_p, \"b s l -> (s b) l\") if num_starts > 1 else log_p\n",
        "        mask = rearrange(mask, \"b s l -> (s b) l\") if num_starts > 1 else mask\n",
        "        return log_p, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MatNetInitEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_dim, one_hot_seed_cnt):\n",
        "        super(MatNetInitEmbedding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.one_hot_seed_cnt = one_hot_seed_cnt\n",
        "\n",
        "    def forward(self, td):\n",
        "        # Generate initial embeddings: [batch, node, node]\n",
        "        cost_mat = td[\"cost_matrix\"]\n",
        "        batch_size = cost_mat.size(0)\n",
        "        node_cnt = cost_mat.size(1)\n",
        "        row_emb = torch.zeros((batch_size, node_cnt, self.embedding_dim), device=cost_mat.device)\n",
        "        col_emb = torch.zeros((batch_size, node_cnt, self.embedding_dim), device=cost_mat.device)\n",
        "        # randomize col_emb: we refactor with topk\n",
        "        rand = torch.rand(batch_size, self.one_hot_seed_cnt, device=cost_mat.device)\n",
        "        _, rand_idx = rand.topk(node_cnt, dim=1, largest=False)\n",
        "        b_idx, n_idx = torch.meshgrid(torch.arange(batch_size, device=cost_mat.device),\n",
        "                                    torch.arange(node_cnt, device=cost_mat.device))\n",
        "        col_emb[b_idx, n_idx, rand_idx] = 1\n",
        "        return row_emb, col_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([100, 50, 128]), torch.Size([100, 50, 128]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = MatNetInitEmbedding(128, 50)\n",
        "\n",
        "td = TensorDict({\"cost_matrix\": torch.randn(100, 50, 50)}, batch_size=100)\n",
        "a, b = embedding(td)\n",
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MatNetPolicy(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: EnvBase,\n",
        "        encoder: nn.Module = None,\n",
        "        decoder: nn.Module = None,\n",
        "        embedding_dim: int = 256,\n",
        "        num_starts: int = 10,\n",
        "        one_hot_seed_cnt: int = 20,\n",
        "        num_encode_layers: int = 5,\n",
        "        num_heads: int = 16,\n",
        "        mask_inner: bool = True,\n",
        "        train_decode_type: str = \"sampling\",\n",
        "        val_decode_type: str = \"greedy\",\n",
        "        test_decode_type: str = \"greedy\",\n",
        "        **unused_kwargs\n",
        "    ):\n",
        "        super(MatNetPolicy, self).__init__()\n",
        "\n",
        "        if len(unused_kwargs) > 0:\n",
        "            print(\"Unused kwargs found in MatNetPolicy init: \", unused_kwargs)\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "        self.init_embedding = MatNetInitEmbedding(embedding_dim, one_hot_seed_cnt)\n",
        "\n",
        "        self.encoder = (\n",
        "            MatNetEncoder(\n",
        "                num_heads=num_heads,\n",
        "                embed_dim=embedding_dim,\n",
        "                num_layers=num_encode_layers,\n",
        "            )\n",
        "            if encoder is None\n",
        "            else encoder\n",
        "        )\n",
        "\n",
        "        self.decoder = (\n",
        "            Decoder(\n",
        "                env,\n",
        "                embedding_dim,\n",
        "                num_heads,\n",
        "                num_starts=num_starts,\n",
        "                mask_inner=mask_inner,\n",
        "            )\n",
        "            if decoder is None\n",
        "            else decoder\n",
        "        )\n",
        "        self.num_starts = num_starts\n",
        "        self.train_decode_type = train_decode_type\n",
        "        self.val_decode_type = val_decode_type\n",
        "        self.test_decode_type = test_decode_type\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        td: TensorDict,\n",
        "        phase: str = \"train\",\n",
        "        return_actions: bool = False,\n",
        "        **decoder_kwargs,\n",
        "    ) -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        row_emb, col_emb = self.init_embedding(td)\n",
        "        encoded_inputs = self.encoder(row_emb, col_emb, td[\"cost_matrix\"])\n",
        "        \n",
        "        # Get decode type depending on phase\n",
        "        if decoder_kwargs.get(\"decode_type\", None) is None:\n",
        "            decoder_kwargs[\"decode_type\"] = getattr(self, f\"{phase}_decode_type\")\n",
        "\n",
        "        # Main rollout\n",
        "        log_p, actions, td = self.decoder(td, encoded_inputs, **decoder_kwargs)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        ll = get_log_likelihood(log_p, actions, td.get(\"mask\", None))\n",
        "        out = {\n",
        "            \"reward\": td[\"reward\"],\n",
        "            \"log_likelihood\": ll,\n",
        "            \"actions\": actions if return_actions else None,\n",
        "        }\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test policy only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7eff8c1f45b0>>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rich.traceback import install\n",
        "install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reward': tensor([-2.7901, -3.0211, -4.2714,  ..., -2.7211, -3.7595, -3.9496],\n",
            "       device='cuda:0'), 'log_likelihood': tensor([-40.8242, -34.1915, -34.9612,  ..., -34.8064, -35.7479, -31.4031],\n",
            "       device='cuda:0', grad_fn=<SumBackward1>), 'actions': None}\n"
          ]
        }
      ],
      "source": [
        "env = ATSPEnv(num_loc=20)\n",
        "\n",
        "dataset = env.dataset(batch_size=[10000])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,  # no need to shuffle, we're resampling every epoch\n",
        "    num_workers=0,\n",
        "    collate_fn=tensordict_collate_fn,\n",
        ")\n",
        "\n",
        "policy = MatNetPolicy(\n",
        "    env,\n",
        "    num_starts=20,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "td = next(iter(dataloader)).to(\"cuda\")\n",
        "td = env.reset(td)\n",
        "\n",
        "out = policy(td, decode_type=\"sampling\")\n",
        "\n",
        "print(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create full MatNet: `env` + `policy` + `baseline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from typing import Any, Optional, Tuple, Union\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "from rl4co.utils.lightning import get_lightning_device\n",
        "from rl4co.models.rl.reinforce.baselines import WarmupBaseline, RolloutBaseline, ExponentialBaseline, SharedBaseline\n",
        "from rl4co.models.zoo.pomo import POMO\n",
        "\n",
        "\n",
        "class MatNet(POMO):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        policy=None,\n",
        "        baseline=None,\n",
        "        num_starts=10,\n",
        "        num_augment=0,\n",
        "        **policy_kwargs\n",
        "    ):\n",
        "        super(POMO, self).__init__(env, policy, baseline)\n",
        "        self.policy = (\n",
        "            MatNetPolicy(self.env, num_starts=num_starts, **policy_kwargs)\n",
        "            if policy is None\n",
        "            else policy\n",
        "        )\n",
        "\n",
        "        self.baseline = SharedBaseline() if baseline is None else baseline\n",
        "\n",
        "        # POMO parameters\n",
        "        self.num_augment = num_augment\n",
        "        self.augment = None # TODO: remove from POMO?\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reward': tensor([-4.0538, -4.0305, -3.1932,  ..., -3.9774, -3.8262, -3.2630],\n",
            "       device='cuda:0'), 'log_likelihood': tensor([-33.7423, -40.1478, -33.1871,  ..., -34.4182, -31.3874, -35.5232],\n",
            "       device='cuda:0', grad_fn=<SumBackward1>), 'actions': None, 'loss': tensor(0.0009, device='cuda:0', grad_fn=<SubBackward0>), 'reinforce_loss': tensor(0.0009, device='cuda:0', grad_fn=<NegBackward0>), 'bl_loss': 0, 'bl_val': tensor([[-4.0845],\n",
            "        [-4.1233],\n",
            "        [-3.5481],\n",
            "        [-3.4527],\n",
            "        [-2.4980],\n",
            "        [-3.0476],\n",
            "        [-4.1127],\n",
            "        [-3.8137],\n",
            "        [-3.4986],\n",
            "        [-3.9167],\n",
            "        [-4.8775],\n",
            "        [-2.4045],\n",
            "        [-3.0212],\n",
            "        [-4.1176],\n",
            "        [-3.0078],\n",
            "        [-3.0771],\n",
            "        [-4.5094],\n",
            "        [-3.5632],\n",
            "        [-3.5146],\n",
            "        [-3.9960],\n",
            "        [-3.2412],\n",
            "        [-4.0907],\n",
            "        [-2.6833],\n",
            "        [-3.0754],\n",
            "        [-2.9327],\n",
            "        [-3.5346],\n",
            "        [-3.1042],\n",
            "        [-3.6088],\n",
            "        [-3.1072],\n",
            "        [-3.2694],\n",
            "        [-3.4744],\n",
            "        [-2.5044],\n",
            "        [-3.2107],\n",
            "        [-3.3926],\n",
            "        [-3.4269],\n",
            "        [-2.8668],\n",
            "        [-3.5999],\n",
            "        [-4.0757],\n",
            "        [-3.0759],\n",
            "        [-3.0254],\n",
            "        [-4.3744],\n",
            "        [-2.8078],\n",
            "        [-4.0364],\n",
            "        [-3.4614],\n",
            "        [-3.8439],\n",
            "        [-3.4159],\n",
            "        [-3.4362],\n",
            "        [-2.9227],\n",
            "        [-3.5986],\n",
            "        [-3.5790],\n",
            "        [-3.6057],\n",
            "        [-2.9991],\n",
            "        [-2.9048],\n",
            "        [-4.0722],\n",
            "        [-3.3468],\n",
            "        [-4.4167],\n",
            "        [-3.3706],\n",
            "        [-4.2522],\n",
            "        [-3.7112],\n",
            "        [-3.2126],\n",
            "        [-2.9335],\n",
            "        [-4.4419],\n",
            "        [-3.9319],\n",
            "        [-3.3318]], device='cuda:0'), 'max_reward': tensor([-3.5438, -3.6766, -3.1408, -3.0060, -2.2307, -2.4750, -3.4019, -3.3360,\n",
            "        -3.1262, -3.4652, -4.3451, -2.1047, -2.6979, -3.4608, -2.6164, -2.7195,\n",
            "        -3.8927, -3.0657, -2.9995, -3.7076, -2.4893, -3.3084, -2.2602, -2.6318,\n",
            "        -2.5997, -3.1544, -2.7692, -3.2703, -2.7595, -2.7997, -3.1644, -2.2239,\n",
            "        -2.7862, -2.6707, -2.7311, -2.5647, -3.3339, -3.5349, -2.7589, -2.5299,\n",
            "        -3.8768, -2.3533, -3.4058, -2.8053, -3.3812, -2.9008, -2.6354, -2.5589,\n",
            "        -3.2404, -3.2027, -3.3789, -2.6650, -2.5096, -3.5909, -2.9517, -3.9252,\n",
            "        -2.9947, -3.7872, -3.2801, -2.6705, -2.5721, -3.9774, -3.5404, -2.9428],\n",
            "       device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = MatNet(\n",
        "    env,\n",
        "    policy,\n",
        "    # baseline=baseline,\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "td = next(iter(dataloader)).to(\"cuda\")\n",
        "td = env.reset(td)\n",
        "\n",
        "out = model(td, decode_type=\"sampling\")\n",
        "\n",
        "print(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "config = DictConfig(\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"train_size\": 100000, # with 1 epochs, this is 1k samples\n",
        "            \"val_size\": 10000, \n",
        "            \"batch_size\": 64, #64,\n",
        "        },\n",
        "        \"optim\": {\n",
        "            \"lr\": 1e-4,\n",
        "            \"weight_decay\": 1e-6,\n",
        "        },\n",
        "        \"num_epochs\": 10,        \n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "lit_module = RL4COLitModule(cfg=config, env=env, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val_file not set. Generating dataset instead\n",
            "test_file not set. Generating dataset instead\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "No optimizer specified, using default\n",
            "\n",
            "  | Name  | Type    | Params\n",
            "----------------------------------\n",
            "0 | env   | ATSPEnv | 0     \n",
            "1 | model | MatNet  | 5.7 M \n",
            "----------------------------------\n",
            "5.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.7 M     Total params\n",
            "22.670    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7271a0139514fe284f8f42aa0f76165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9c21ab888ea41098433ab8db8952c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48b399ced0974441bfafefec6541c996",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=config.num_epochs, # only few epochs\n",
        "    accelerator=\"gpu\", # use GPU if available, else you can use others as \"cpu\"\n",
        "    devices=[0], # GPU number, or multiple GPUs [0, 1, 2, ...]\n",
        "    logger=None, # can replace with WandbLogger, TensorBoardLogger, etc.\n",
        "    precision=\"16-mixed\", # Lightning will handle faster training with mixed precision\n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        "    reload_dataloaders_every_n_epochs=1, # necessary for sampling new data\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(lit_module)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
