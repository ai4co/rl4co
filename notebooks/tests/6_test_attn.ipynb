{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "# from torch.nn.functional import scaled_dot_product_attention\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_heads,\n",
    "            input_dim,\n",
    "            embed_dim,\n",
    "            val_dim=None,\n",
    "            key_dim=None\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        if val_dim is None:\n",
    "            val_dim = embed_dim // n_heads\n",
    "        if key_dim is None:\n",
    "            key_dim = val_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.val_dim = val_dim\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n",
    "        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n",
    "        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n",
    "\n",
    "        self.W_out = nn.Parameter(torch.Tensor(n_heads, val_dim, embed_dim))\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "\n",
    "        for param in self.parameters():\n",
    "            stdv = 1. / math.sqrt(param.size(-1))\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, q, h=None, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param q: queries (batch_size, n_query, input_dim)\n",
    "        :param h: data (batch_size, graph_size, input_dim)\n",
    "        :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n",
    "        Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if h is None:\n",
    "            h = q  # compute self-attention\n",
    "\n",
    "        # h should be (batch_size, graph_size, input_dim)\n",
    "        batch_size, graph_size, input_dim = h.size()\n",
    "        n_query = q.size(1)\n",
    "        assert q.size(0) == batch_size\n",
    "        assert q.size(2) == input_dim\n",
    "        assert input_dim == self.input_dim, \"Wrong embedding dimension of input\"\n",
    "\n",
    "        hflat = h.contiguous().view(-1, input_dim)\n",
    "        qflat = q.contiguous().view(-1, input_dim)\n",
    "\n",
    "        # last dimension can be different for keys and values\n",
    "        shp = (self.n_heads, batch_size, graph_size, -1)\n",
    "        shp_q = (self.n_heads, batch_size, n_query, -1)\n",
    "\n",
    "        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n",
    "        Q = torch.matmul(qflat, self.W_query).view(shp_q)\n",
    "        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n",
    "        K = torch.matmul(hflat, self.W_key).view(shp)\n",
    "        V = torch.matmul(hflat, self.W_val).view(shp)\n",
    "\n",
    "        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n",
    "        # compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n",
    "        compatibility = torch.matmul(Q, K.transpose(2, 3))\n",
    "\n",
    "        # Optionally apply mask to prevent attention\n",
    "        if mask is not None:\n",
    "            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n",
    "            compatibility[mask] = -np.inf\n",
    "\n",
    "        attn = torch.softmax(compatibility, dim=-1)\n",
    "\n",
    "        # If there are nodes with no neighbours then softmax returns nan so we fix them to 0\n",
    "        if mask is not None:\n",
    "            attnc = attn.clone()\n",
    "            attnc[mask] = 0\n",
    "            attn = attnc\n",
    "\n",
    "        heads = torch.matmul(attn, V)\n",
    "\n",
    "        out = torch.mm(\n",
    "            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n",
    "            self.W_out.view(-1, self.embed_dim)\n",
    "        ).view(batch_size, n_query, self.embed_dim)\n",
    "\n",
    "        # Alternative:\n",
    "        # headst = heads.transpose(0, 1)  # swap the dimensions for batch and heads to align it for the matmul\n",
    "        # # proj_h = torch.einsum('bhni,hij->bhnj', headst, self.W_out)\n",
    "        # projected_heads = torch.matmul(headst, self.W_out)\n",
    "        # out = torch.sum(projected_heads, dim=1)  # sum across heads\n",
    "\n",
    "        # Or:\n",
    "        # out = torch.einsum('hbni,hij->bnj', heads, self.W_out)\n",
    "\n",
    "        return out, heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35355339059327373\n"
     ]
    }
   ],
   "source": [
    "N_HEADS = 4\n",
    "INPUT_DIM = EMBED_DIM = 32\n",
    "BATCH = 3\n",
    "N_NODES = 5\n",
    "\n",
    "x = torch.randn(BATCH, N_NODES, EMBED_DIM)\n",
    "original_attn = MultiHeadAttention(n_heads=N_HEADS, input_dim=INPUT_DIM, embed_dim=EMBED_DIM)\n",
    "print(original_attn.norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH, N_NODES, EMBED_DIM)\n",
    "x = torch.ones(BATCH, N_NODES, EMBED_DIM)\n",
    "x[:, 0, :] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wqkv = nn.Linear(EMBED_DIM, 3 * EMBED_DIM, bias=False)\n",
    "Wqkv.weight.data = torch.ones_like(Wqkv.weight.data)\n",
    "\n",
    "out_proj = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)\n",
    "out_proj.weight.data = torch.ones_like(out_proj.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "        Q, K, V, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None\n",
    "    ):\n",
    "        print(\"Here\")\n",
    "        \"\"\"Simple Scaled Dot-Product Attention in PyTorch without Flash Attention\"\"\"\n",
    "        if scale is None:\n",
    "            scale = Q.size(-1) ** -0.5  # scale factor\n",
    "\n",
    "        print(f\"Scale: {scale}\")\n",
    "        # compute the attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        return torch.matmul(attn_probs, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Scale: 0.3535533905932738\n"
     ]
    }
   ],
   "source": [
    "q, k, v = rearrange(Wqkv(x), \"b s (three h d) -> three b s h d\", three=3, h=N_HEADS).unbind(dim=0)\n",
    "o = scaled_dot_product_attention(q, k, v)\n",
    "h = out_proj(rearrange(o, \"b s h d -> b s (h d)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35355339059327373"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_attn.norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5]) torch.Size([4, 3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "original_attn = MultiHeadAttention(n_heads=N_HEADS, input_dim=INPUT_DIM, embed_dim=EMBED_DIM)\n",
    "original_attn.W_query.data = torch.ones_like(original_attn.W_query.data)\n",
    "original_attn.W_key.data = torch.ones_like(original_attn.W_key.data)\n",
    "original_attn.W_val.data = torch.ones_like(original_attn.W_val.data)\n",
    "original_attn.W_out.data = torch.ones_like(original_attn.W_out.data)\n",
    "\n",
    "h2, o2 = original_attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4, 8])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 5, 8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.6000, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o.permute(2,0,1,3) - o2).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-819.1998, -819.1998, -819.1998, -819.1998, -819.1998, -819.1998,\n",
       "        -819.1998, -819.1998, -819.1998, -819.1998, -819.1998, -819.1998,\n",
       "        -819.1998, -819.1998, -819.1998, -819.1998, -819.1998, -819.1998,\n",
       "        -819.1998, -819.1998, -819.1998, -819.1998, -819.1998, -819.1998,\n",
       "        -819.1998, -819.1998, -819.1998, -819.1998, -819.1998, -819.1998,\n",
       "        -819.1998, -819.1998], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, n = 0,0\n",
    "\n",
    "h[b, n] - h2[b, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_attn = nn.MultiheadAttention(embed_dim=EMBED_DIM, num_heads=N_HEADS, \n",
    "                                     bias=False, batch_first=True)\n",
    "pytorch_attn.in_proj_weight.data = torch.ones_like(pytorch_attn.in_proj_weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4, 8])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3, o3 = pytorch_attn(q.view(BATCH, N_NODES, -1), k.view(BATCH, N_NODES, -1), v.view(BATCH, N_NODES, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1155.6492, -444.0484, -230.0319,  -96.4411,  376.5367,  498.6359,\n",
       "         -151.0114, -218.2406, -385.9967,  161.0141, -766.5836, -272.5924,\n",
       "         -295.6412,  847.5152, -744.7882, -551.1451,  257.4138,  183.6791,\n",
       "         -187.7901,  262.6219, -382.0503, 1638.3092,  -46.4304, -340.5738,\n",
       "          143.1985, -323.4807, -705.9189, 1044.2151,   28.0299,  252.7086,\n",
       "          769.5617, -720.9847],\n",
       "        [1444.5614, -555.0605, -287.5400, -120.5514,  470.6707,  623.2948,\n",
       "         -188.7643, -272.8008, -482.4958,  201.2676, -958.2294, -340.7405,\n",
       "         -369.5516, 1059.3940, -930.9854, -688.9313,  321.7672,  229.5989,\n",
       "         -234.7376,  328.2773, -477.5630, 2047.8865,  -58.0379, -425.7173,\n",
       "          178.9982, -404.3507, -882.3987, 1305.2687,   35.0373,  315.8858,\n",
       "          961.9521, -901.2307],\n",
       "        [1444.5614, -555.0605, -287.5400, -120.5514,  470.6707,  623.2948,\n",
       "         -188.7643, -272.8008, -482.4958,  201.2676, -958.2294, -340.7405,\n",
       "         -369.5516, 1059.3940, -930.9854, -688.9313,  321.7672,  229.5989,\n",
       "         -234.7376,  328.2773, -477.5630, 2047.8865,  -58.0379, -425.7173,\n",
       "          178.9982, -404.3507, -882.3987, 1305.2687,   35.0373,  315.8858,\n",
       "          961.9521, -901.2307],\n",
       "        [1444.5614, -555.0605, -287.5400, -120.5514,  470.6707,  623.2948,\n",
       "         -188.7643, -272.8008, -482.4958,  201.2676, -958.2294, -340.7405,\n",
       "         -369.5516, 1059.3940, -930.9854, -688.9313,  321.7672,  229.5989,\n",
       "         -234.7376,  328.2773, -477.5630, 2047.8865,  -58.0379, -425.7173,\n",
       "          178.9982, -404.3507, -882.3987, 1305.2687,   35.0373,  315.8858,\n",
       "          961.9521, -901.2307],\n",
       "        [1444.5613, -555.0605, -287.5400, -120.5514,  470.6708,  623.2948,\n",
       "         -188.7643, -272.8007, -482.4958,  201.2676, -958.2294, -340.7405,\n",
       "         -369.5516, 1059.3940, -930.9852, -688.9312,  321.7672,  229.5989,\n",
       "         -234.7376,  328.2774, -477.5630, 2047.8862,  -58.0379, -425.7173,\n",
       "          178.9982, -404.3508, -882.3988, 1305.2687,   35.0372,  315.8858,\n",
       "          961.9520, -901.2307]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 819.1998,  819.1998,  819.1998,  819.1998,  819.1998,  819.1998,\n",
       "          819.1998,  819.1998,  819.1998,  819.1998,  819.1998,  819.1998,\n",
       "          819.1998,  819.1998,  819.1998,  819.1998,  819.1998,  819.1998,\n",
       "          819.1998,  819.1998,  819.1998,  819.1998,  819.1998,  819.1998,\n",
       "          819.1998,  819.1998,  819.1998,  819.1998,  819.1998,  819.1998,\n",
       "          819.1998,  819.1998],\n",
       "        [1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000],\n",
       "        [1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000],\n",
       "        [1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000],\n",
       "        [1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000, 1024.0000,\n",
       "         1024.0000, 1024.0000]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
