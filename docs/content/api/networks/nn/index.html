
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Official documentation for RL4CO">
      
      
      
        <link rel="canonical" href="https://ai4co.github.io/rl4co/docs/content/api/networks/nn/">
      
      
        <link rel="prev" href="../base_policies/">
      
      
        <link rel="next" href="../env_embeddings/">
      
      
      <link rel="icon" href="../../../../assets/figs/rl4co-logo.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Neural Network Modules - RL4CO</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Noto Sans";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/mkdocstrings.css">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#neural-network-modules" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="RL4CO" class="md-header__button md-logo" aria-label="RL4CO" data-md-component="logo">
      
  <img src="../../../../assets/figs/rl4co-logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            RL4CO
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Neural Network Modules
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="custom"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ai4co/rl4co/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    ai4co/rl4co
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../start/installation/" class="md-tabs__link">
          
  
  
    
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../intro/intro/" class="md-tabs__link">
          
  
  
    
  
  Overview

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../examples/1-quickstart/" class="md-tabs__link">
          
  
  
    
  
  Tutorials

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../envs/base/" class="md-tabs__link">
          
  
  
    
  
  API Reference

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../general/contribute/" class="md-tabs__link">
          
  
  
    
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="RL4CO" class="md-nav__button md-logo" aria-label="RL4CO" data-md-component="logo">
      
  <img src="../../../../assets/figs/rl4co-logo.svg" alt="logo">

    </a>
    RL4CO
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ai4co/rl4co/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    ai4co/rl4co
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../start/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/1-quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RL4CO Quickstart Notebook
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../start/hydra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training with Hydra
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Overview
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../intro/intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../intro/environments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Environments
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../intro/policies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Policies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../intro/rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RL Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Main
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Main
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/1-quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RL4CO Quickstart Notebook
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/2-full-training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training: Checkpoints, Logging, and Callbacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/3-creating-new-env-model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    New Environment: Creating and Modeling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Modeling
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Modeling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/modeling/1-decoding-strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RL4CO Decoding Strategies Notebook
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/modeling/2-transductive-methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transductive Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/modeling/3-change-encoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Encoder Customization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Advanced
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/advanced/1-hydra-config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hydra Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/advanced/3-local-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Local Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/other/3-data-generator-distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generating data in RL4CO
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_4" >
        
          
          <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Routing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            Routing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/other/1-mtvrp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MTVRP: Multi-task VRP environment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/datasets/1-test-on-tsplib/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Testing Model on TSPLib
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/datasets/2-test-on-cvrplib/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Testing Model on VRPLib
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Scheduling
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Scheduling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/other/2-scheduling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Solving the Flexible Job-Shop Scheduling Problem (FJSP)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Environments
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            Environments
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../envs/base/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Base Classes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../envs/routing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Routing Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../envs/scheduling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduling Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../envs/eda/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDA Problems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../envs/graph/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Graph Problems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" checked>
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../base_policies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Policy Base Classes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Neural Network Modules
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Neural Network Modules
    
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="Page contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Page contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#critic-network" class="md-nav__link">
    <span class="md-ellipsis">
      Critic Network
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.rl.common.critic.CriticNetwork" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CriticNetwork
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CriticNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.rl.common.critic.CriticNetwork.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Graph Neural Networks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.MultiHeadAttentionLayer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadAttentionLayer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.GraphAttentionNetwork" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;GraphAttentionNetwork
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" GraphAttentionNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.GraphAttentionNetwork.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.gcn.GCNEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;GCNEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" GCNEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.graph.gcn.GCNEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.mpnn.MessagePassingEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MessagePassingEncoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanisms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MultiHeadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCrossAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadCrossAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PointerAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PointerAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttnMoE" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PointerAttnMoE
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCompat" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadCompat
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MultiHeadCompat">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCompat.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PolyNetAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PolyNetAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PolyNetAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.PolyNetAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.scaled_dot_product_attention_simple" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;scaled_dot_product_attention_simple
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-layer-perceptron" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Layer Perceptron
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.mlp.MLP" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations" class="md-nav__link">
    <span class="md-ellipsis">
      Operations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.ops.PositionalEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositionalEncoding
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.ops.PositionalEncoding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.ops.RandomEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;RandomEncoding
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../env_embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Environment Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    RL Algorithms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            RL Algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rl/base/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Base Classes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rl/reinforce/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    REINFORCE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rl/ppo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rl/a2c/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    A2C
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Zoo
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Zoo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zoo/constructive_ar/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Constructive AR Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zoo/constructive_nar/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Constructive NAR Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zoo/improvement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Improvement Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zoo/transductive/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transductive Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_5" >
        
          
          <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Additional APIs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            Additional APIs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tasks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Train and Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoding Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            About
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../general/contribute/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing to RL4CO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../general/faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../general/paper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paper and Citation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../general/licensing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    License and Usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../general/ai4co/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Join AI4CO
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="Page contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Page contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#critic-network" class="md-nav__link">
    <span class="md-ellipsis">
      Critic Network
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.rl.common.critic.CriticNetwork" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CriticNetwork
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CriticNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.rl.common.critic.CriticNetwork.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Graph Neural Networks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.MultiHeadAttentionLayer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadAttentionLayer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.GraphAttentionNetwork" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;GraphAttentionNetwork
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" GraphAttentionNetwork">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.graph.attnnet.GraphAttentionNetwork.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.gcn.GCNEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;GCNEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" GCNEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.graph.gcn.GCNEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.graph.mpnn.MessagePassingEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MessagePassingEncoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanisms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MultiHeadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCrossAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadCrossAttention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PointerAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PointerAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PointerAttnMoE" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PointerAttnMoE
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCompat" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MultiHeadCompat
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" MultiHeadCompat">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.MultiHeadCompat.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.PolyNetAttention" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PolyNetAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PolyNetAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.attention.PolyNetAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.attention.scaled_dot_product_attention_simple" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;scaled_dot_product_attention_simple
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-layer-perceptron" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Layer Perceptron
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.mlp.MLP" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations" class="md-nav__link">
    <span class="md-ellipsis">
      Operations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.ops.PositionalEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositionalEncoding
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#models.nn.ops.PositionalEncoding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models.nn.ops.RandomEncoding" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;RandomEncoding
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                


                  


  
  


<h1 id="neural-network-modules">Neural Network Modules<a class="headerlink" href="#neural-network-modules" title="Permanent link">&para;</a></h1>
<h2 id="critic-network">Critic Network<a class="headerlink" href="#critic-network" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="models.rl.common.critic.CriticNetwork" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">CriticNetwork</span>


<a href="#models.rl.common.critic.CriticNetwork" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">CriticNetwork</span><span class="p">(</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
    <span class="n">value_head</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">customized</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>Create a critic network given an encoder (e.g. as the one in the policy network)
with a value head to transform the embeddings to a scalar value.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>encoder</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>)
          –
          <div class="doc-md-description">
            <p>Encoder module to encode the input</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>value_head</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Value head to transform the embeddings to a scalar value</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>128</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension of the embeddings of the value head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>hidden_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>512</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension of the hidden layer of the value head</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.rl.common.critic.CriticNetwork.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Forward pass of the critic network: encode the imput in embedding space and return the value</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/rl/common/critic.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">value_head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">customized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CriticNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
    <span class="k">if</span> <span class="n">value_head</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># check if embed dim of encoder is different, if so, use it</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="s2">&quot;embed_dim&quot;</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="n">embed_dim</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Found encoder with different embed_dim </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2"> than the value head </span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">. Using encoder embed_dim for value head.&quot;</span>
            <span class="p">)</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="s2">&quot;embed_dim&quot;</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">value_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value_head</span> <span class="o">=</span> <span class="n">value_head</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">customized</span> <span class="o">=</span> <span class="n">customized</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.rl.common.critic.CriticNetwork.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.rl.common.critic.CriticNetwork.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;tensordict.TensorDict&lt;/code&gt;" href="https://pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict">TensorDict</a></span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of the critic network: encode the imput in embedding space and return the value</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>x</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a> | <a class="autorefs autorefs-external" title="&lt;code&gt;tensordict.TensorDict&lt;/code&gt;" href="https://pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict">TensorDict</a></code>)
          –
          <div class="doc-md-description">
            <p>Input containing the environment state. Can be a Tensor or a TensorDict</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
          –
          <div class="doc-md-description">
            <p>Value of the input state</p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/rl/common/critic.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="n">TensorDict</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass of the critic network: encode the imput in embedding space and return the value</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input containing the environment state. Can be a Tensor or a TensorDict</span>

<span class="sd">    Returns:</span>
<span class="sd">        Value of the input state</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">customized</span><span class="p">:</span>  <span class="c1"># fir for most of costructive tasks</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, N, embed_dim] -&gt; [batch_size, N]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_head</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [batch_size, N] -&gt; [batch_size]</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># custimized encoder and value head with hidden input</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, N, embed_dim] -&gt; [batch_size, N]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_head</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="graph-neural-networks">Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="models.nn.graph.attnnet.MultiHeadAttentionLayer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MultiHeadAttentionLayer</span>


<a href="#models.nn.graph.attnnet.MultiHeadAttentionLayer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MultiHeadAttentionLayer</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">feedforward_hidden</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Sequential&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">Sequential</a></code></p>


        <p>Multi-Head Attention Layer with normalization and feed-forward layer</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>dimension of the embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>number of heads in the MHA</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>feedforward_hidden</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>512</code>
)
          –
          <div class="doc-md-description">
            <p>dimension of the hidden layer in the feed-forward layer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>normalization</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>]</code>, default:
                  <code>&#39;batch&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>type of normalization to use (batch, layer, none)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>moe_kwargs</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Keyword arguments for MoE</p>
          </div>
        </li>
    </ul>











                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/graph/attnnet.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">feedforward_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">num_neurons</span> <span class="o">=</span> <span class="p">[</span><span class="n">feedforward_hidden</span><span class="p">]</span> <span class="k">if</span> <span class="n">feedforward_hidden</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">moe_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ffn</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="n">num_neurons</span><span class="p">,</span> <span class="o">**</span><span class="n">moe_kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ffn</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="n">num_neurons</span><span class="p">,</span> <span class="n">hidden_act</span><span class="o">=</span><span class="s2">&quot;ReLU&quot;</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttentionLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">SkipConnection</span><span class="p">(</span>
            <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">sdpa_fn</span><span class="o">=</span><span class="n">sdpa_fn</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">Normalization</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">normalization</span><span class="p">),</span>
        <span class="n">SkipConnection</span><span class="p">(</span><span class="n">ffn</span><span class="p">),</span>
        <span class="n">Normalization</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">normalization</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.graph.attnnet.GraphAttentionNetwork" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">GraphAttentionNetwork</span>


<a href="#models.nn.graph.attnnet.GraphAttentionNetwork" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">GraphAttentionNetwork</span><span class="p">(</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span>
    <span class="n">feedforward_hidden</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>Graph Attention Network to encode embeddings with a series of MHA layers consisting of a MHA layer,
normalization, feed-forward layer, and normalization. Similar to Transformer encoder, as used in Kool et al. (2019).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads in the MHA</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>dimension of the embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_layers</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of MHA layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>normalization</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>, default:
                  <code>&#39;batch&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>type of normalization to use (batch, layer, none)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>feedforward_hidden</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>512</code>
)
          –
          <div class="doc-md-description">
            <p>dimension of the hidden layer in the feed-forward layer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>moe_kwargs</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Keyword arguments for MoE</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.graph.attnnet.GraphAttentionNetwork.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Forward pass of the encoder</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/graph/attnnet.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span>
    <span class="n">feedforward_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GraphAttentionNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="o">*</span><span class="p">(</span>
            <span class="n">MultiHeadAttentionLayer</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
                <span class="n">feedforward_hidden</span><span class="o">=</span><span class="n">feedforward_hidden</span><span class="p">,</span>
                <span class="n">normalization</span><span class="o">=</span><span class="n">normalization</span><span class="p">,</span>
                <span class="n">sdpa_fn</span><span class="o">=</span><span class="n">sdpa_fn</span><span class="p">,</span>
                <span class="n">moe_kwargs</span><span class="o">=</span><span class="n">moe_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.graph.attnnet.GraphAttentionNetwork.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.graph.attnnet.GraphAttentionNetwork.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of the encoder</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>x</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>)
          –
          <div class="doc-md-description">
            <p>[batch_size, graph_size, embed_dim] initial embeddings to process</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>mask</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>[batch_size, graph_size, graph_size] mask for the input embeddings. Unused for now.</p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/graph/attnnet.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass of the encoder</span>

<span class="sd">    Args:</span>
<span class="sd">        x: [batch_size, graph_size, embed_dim] initial embeddings to process</span>
<span class="sd">        mask: [batch_size, graph_size, graph_size] mask for the input embeddings. Unused for now.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Mask not yet supported!&quot;</span>
    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.graph.gcn.GCNEncoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">GCNEncoder</span>


<a href="#models.nn.graph.gcn.GCNEncoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">GCNEncoder</span><span class="p">(</span>
    <span class="n">env_name</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">init_embedding</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">edge_idx_fn</span><span class="p">:</span> <span class="n"><span title="models.nn.graph.gcn.EdgeIndexFnSignature">EdgeIndexFnSignature</span></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>Graph Convolutional Network to encode embeddings with a series of GCN
layers from the pytorch geometric package</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>dimension of the embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_nodes</code></b>
          –
          <div class="doc-md-description">
            <p>number of nodes in the graph</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_gcn_layer</code></b>
          –
          <div class="doc-md-description">
            <p>number of GCN layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>self_loop</code></b>
          –
          <div class="doc-md-description">
            <p>whether to add self loop in the graph</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>residual</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use residual connection</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.graph.gcn.GCNEncoder.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Forward pass of the encoder.</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/graph/gcn.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">init_embedding</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">edge_idx_fn</span><span class="p">:</span> <span class="n">EdgeIndexFnSignature</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">env_name</span> <span class="o">=</span> <span class="n">env_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_embedding</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">env_init_embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_name</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="n">embed_dim</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">init_embedding</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_embedding</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">edge_idx_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No edge indices passed. Assume a fully connected graph&quot;</span><span class="p">)</span>
        <span class="n">edge_idx_fn</span> <span class="o">=</span> <span class="n">edge_idx_fn_wrapper</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">edge_idx_fn</span> <span class="o">=</span> <span class="n">edge_idx_fn</span>

    <span class="c1"># Define the GCN layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gcn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span><span class="n">GCNConv</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.graph.gcn.GCNEncoder.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.graph.gcn.GCNEncoder.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span>
    <span class="n">td</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;tensordict.TensorDict&lt;/code&gt;" href="https://pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict">TensorDict</a></span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Tuple&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Tuple">Tuple</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass of the encoder.
Transform the input TensorDict into a latent representation.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>td</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;tensordict.TensorDict&lt;/code&gt;" href="https://pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict">TensorDict</a></code>)
          –
          <div class="doc-md-description">
            <p>Input TensorDict containing the environment state</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>mask</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a> | None</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Mask to apply to the attention</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>h</code></b> (              <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
)          –
          <div class="doc-md-description">
            <p>Latent representation of the input</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<b><code>init_h</code></b> (              <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
)          –
          <div class="doc-md-description">
            <p>Initial embedding of the input</p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/graph/gcn.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">td</span><span class="p">:</span> <span class="n">TensorDict</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass of the encoder.</span>
<span class="sd">    Transform the input TensorDict into a latent representation.</span>

<span class="sd">    Args:</span>
<span class="sd">        td: Input TensorDict containing the environment state</span>
<span class="sd">        mask: Mask to apply to the attention</span>

<span class="sd">    Returns:</span>
<span class="sd">        h: Latent representation of the input</span>
<span class="sd">        init_h: Initial embedding of the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Transfer to embedding space</span>
    <span class="n">init_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_embedding</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
    <span class="n">bs</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">emb_dim</span> <span class="o">=</span> <span class="n">init_h</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># (bs*num_nodes, emb_dim)</span>
    <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">init_h</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
    <span class="c1"># shape=(2, num_edges)</span>
    <span class="n">edge_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_idx_fn</span><span class="p">(</span><span class="n">td</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcn_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">update_node_feature</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">update_node_feature</span><span class="p">)</span>
        <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">update_node_feature</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>
        <span class="p">)</span>

    <span class="c1"># last layer without relu activation and dropout</span>
    <span class="n">update_node_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcn_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">update_node_feature</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

    <span class="c1"># De-batch the graph</span>
    <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">update_node_feature</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>

    <span class="c1"># Residual</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="n">update_node_feature</span> <span class="o">=</span> <span class="n">update_node_feature</span> <span class="o">+</span> <span class="n">init_h</span>

    <span class="k">return</span> <span class="n">update_node_feature</span><span class="p">,</span> <span class="n">init_h</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.graph.mpnn.MessagePassingEncoder" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MessagePassingEncoder</span>


<a href="#models.nn.graph.mpnn.MessagePassingEncoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MessagePassingEncoder</span><span class="p">(</span>
    <span class="n">env_name</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">init_embedding</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span>
    <span class="n">self_loop</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>












                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/graph/mpnn.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">init_embedding</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span>
    <span class="n">self_loop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Note:</span>
<span class="sd">        - Support fully connected graph for now.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MessagePassingEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">env_name</span> <span class="o">=</span> <span class="n">env_name</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_embedding</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">env_init_embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_name</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="n">embed_dim</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">init_embedding</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">init_embedding</span>
    <span class="p">)</span>

    <span class="c1"># Generate edge index for a fully connected graph</span>
    <span class="n">adj_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">self_loop</span><span class="p">:</span>
        <span class="n">adj_matrix</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># No self-loops</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">adj_matrix</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># Init message passing models</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mpnn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">MessagePassingLayer</span><span class="p">(</span>
                <span class="n">node_indim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
                <span class="n">node_outdim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
                <span class="n">edge_indim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">edge_outdim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
                <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Record parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_loop</span> <span class="o">=</span> <span class="n">self_loop</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div><h2 id="attention-mechanisms">Attention Mechanisms<a class="headerlink" href="#attention-mechanisms" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.MultiHeadAttention">MultiHeadAttention</a></code></b>
          –
          <div class="doc-md-description">
            <p>PyTorch native implementation of Flash Multi-Head Attention with automatic mixed precision support.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.MultiHeadCrossAttention">MultiHeadCrossAttention</a></code></b>
          –
          <div class="doc-md-description">
            <p>PyTorch native implementation of Flash Multi-Head Cross Attention with automatic mixed precision support.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.PointerAttention">PointerAttention</a></code></b>
          –
          <div class="doc-md-description">
            <p>Calculate logits given query, key and value and logit key.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.PointerAttnMoE">PointerAttnMoE</a></code></b>
          –
          <div class="doc-md-description">
            <p>Calculate logits given query, key and value and logit key.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.MultiHeadCompat">MultiHeadCompat</a></code></b>
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.PolyNetAttention">PolyNetAttention</a></code></b>
          –
          <div class="doc-md-description">
            <p>Calculate logits given query, key and value and logit key.</p>
          </div>
        </li>
    </ul>




<p><span class="doc-section-title">Functions:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.scaled_dot_product_attention_simple">scaled_dot_product_attention_simple</a></code></b>
            –
            <div class="doc-md-description">
              <p>Simple (exact) Scaled Dot-Product Attention in RL4CO without customized kernels (i.e. no Flash Attention).</p>
            </div>
          </li>
    </ul>





  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.MultiHeadAttention" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MultiHeadAttention</span>


<a href="#models.nn.attention.MultiHeadAttention" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MultiHeadAttention</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.dtype&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype">dtype</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>PyTorch native implementation of Flash Multi-Head Attention with automatic mixed precision support.
Uses PyTorch's native <code>scaled_dot_product_attention</code> implementation, available from 2.0</p>


<details class="note" open>
  <summary>Note</summary>
  <p>If <code>scaled_dot_product_attention</code> is not available, use custom implementation of <code>scaled_dot_product_attention</code> without Flash Attention.</p>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>total dimension of the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>bias</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use bias</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>attention_dropout</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>, default:
                  <code>0.0</code>
)
          –
          <div class="doc-md-description">
            <p>dropout rate for attention weights</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>causal</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to apply causal mask to attention scores</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>device</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>torch device</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>dtype</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.dtype&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype">dtype</a></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>torch dtype</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA) implementation</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.MultiHeadAttention.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim)</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">sdpa_fn</span> <span class="k">if</span> <span class="n">sdpa_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">scaled_dot_product_attention</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;self.kdim must be divisible by num_heads&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">%</span> <span class="mi">8</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">&lt;=</span> <span class="mi">128</span>
    <span class="p">),</span> <span class="s2">&quot;Only support head_dim &lt;= 128 and divisible by 8&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.attention.MultiHeadAttention.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.attention.MultiHeadAttention.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim)
attn_mask: bool tensor of shape (batch, seqlen)</p>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim)</span>
<span class="sd">    attn_mask: bool tensor of shape (batch, seqlen)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Project query, key, value</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;b s (three h d) -&gt; three b h s d&quot;</span><span class="p">,</span> <span class="n">three</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
            <span class="k">else</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Scaled dot product attention</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sdpa_fn</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s2">&quot;b h s d -&gt; b s (h d)&quot;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.MultiHeadCrossAttention" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MultiHeadCrossAttention</span>


<a href="#models.nn.attention.MultiHeadCrossAttention" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MultiHeadCrossAttention</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.dtype&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype">dtype</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>PyTorch native implementation of Flash Multi-Head Cross Attention with automatic mixed precision support.
Uses PyTorch's native <code>scaled_dot_product_attention</code> implementation, available from 2.0</p>


<details class="note" open>
  <summary>Note</summary>
  <p>If <code>scaled_dot_product_attention</code> is not available, use custom implementation of <code>scaled_dot_product_attention</code> without Flash Attention.</p>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>total dimension of the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>bias</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use bias</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>attention_dropout</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>, default:
                  <code>0.0</code>
)
          –
          <div class="doc-md-description">
            <p>dropout rate for attention weights</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>device</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>torch device</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>dtype</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.dtype&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype">dtype</a></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>torch dtype</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a> | <a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA)</p>
          </div>
        </li>
    </ul>











                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span> <span class="o">|</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>

    <span class="c1"># Default to `scaled_dot_product_attention` if `sdpa_fn` is not provided</span>
    <span class="k">if</span> <span class="n">sdpa_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">sdpa_fn_wrapper</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">sdpa_fn</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;self.kdim must be divisible by num_heads&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">%</span> <span class="mi">8</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">&lt;=</span> <span class="mi">128</span>
    <span class="p">),</span> <span class="s2">&quot;Only support head_dim &lt;= 128 and divisible by 8&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Wkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.PointerAttention" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">PointerAttention</span>


<a href="#models.nn.attention.PointerAttention" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">PointerAttention</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">mask_inner</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">out_bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_nan</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>Calculate logits given query, key and value and logit key.
This follows the pointer mechanism of Vinyals et al. (2015) (<a href="https://arxiv.org/abs/1506.03134">https://arxiv.org/abs/1506.03134</a>).</p>


<details class="note" open>
  <summary>Note</summary>
  <p>With Flash Attention, masking is not supported</p>
</details>

<details class="performs-the-following" open>
  <summary>Performs the following</summary>
  <ol>
<li>Apply cross attention to get the heads</li>
<li>Project heads to get glimpse</li>
<li>Compute attention score between glimpse and logit key</li>
</ol>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>total dimension of the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>mask_inner</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to mask inner attention</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>linear_bias</code></b>
          –
          <div class="doc-md-description">
            <p>whether to use bias in linear projection</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>check_nan</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to check for NaNs in logits</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a> | <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></code>, default:
                  <code>&#39;default&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA) implementation</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.PointerAttention.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Compute attention logits given query, key, value, logit key and attention mask.</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mask_inner</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">out_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_nan</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PointerAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_inner</span> <span class="o">=</span> <span class="n">mask_inner</span>

    <span class="c1"># Projection - query, key, value already include projections</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">out_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">check_nan</span> <span class="o">=</span> <span class="n">check_nan</span>

    <span class="c1"># Defaults for sdpa_fn implementation</span>
    <span class="c1"># see https://github.com/ai4co/rl4co/issues/228</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sdpa_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sdpa_fn</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span>
        <span class="k">elif</span> <span class="n">sdpa_fn</span> <span class="o">==</span> <span class="s2">&quot;simple&quot;</span><span class="p">:</span>
            <span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention_simple</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown sdpa_fn: </span><span class="si">{</span><span class="n">sdpa_fn</span><span class="si">}</span><span class="s2">. Available options: [&#39;default&#39;, &#39;simple&#39;]&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sdpa_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span>
            <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Using default scaled_dot_product_attention for PointerAttention&quot;</span>
            <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sdpa_fn</span> <span class="o">=</span> <span class="n">sdpa_fn</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.attention.PointerAttention.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.attention.PointerAttention.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">logit_key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute attention logits given query, key, value, logit key and attention mask.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>query</code></b>
          –
          <div class="doc-md-description">
            <p>query tensor of shape [B, ..., L, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>key</code></b>
          –
          <div class="doc-md-description">
            <p>key tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>value</code></b>
          –
          <div class="doc-md-description">
            <p>value tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>logit_key</code></b>
          –
          <div class="doc-md-description">
            <p>logit key tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>attn_mask</code></b>
          –
          <div class="doc-md-description">
            <p>attention mask tensor of shape [B, ..., S]. Note that <code>True</code> means that the value <em>should</em> take part in attention
as described in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">PyTorch Documentation</a></p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">logit_key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute attention logits given query, key, value, logit key and attention mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        query: query tensor of shape [B, ..., L, E]</span>
<span class="sd">        key: key tensor of shape [B, ..., S, E]</span>
<span class="sd">        value: value tensor of shape [B, ..., S, E]</span>
<span class="sd">        logit_key: logit key tensor of shape [B, ..., S, E]</span>
<span class="sd">        attn_mask: attention mask tensor of shape [B, ..., S]. Note that `True` means that the value _should_ take part in attention</span>
<span class="sd">            as described in the [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute inner multi-head attention with no projections.</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inner_mha</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
    <span class="n">glimpse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_out</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>

    <span class="c1"># Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)</span>
    <span class="c1"># bmm is slightly faster than einsum and matmul</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">glimpse</span><span class="p">,</span> <span class="n">logit_key</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
        <span class="o">-</span><span class="mi">2</span>
    <span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">glimpse</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_nan</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Logits contain NaNs&quot;</span>

    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.PointerAttnMoE" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">PointerAttnMoE</span>


<a href="#models.nn.attention.PointerAttnMoE" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">PointerAttnMoE</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">mask_inner</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">out_bias</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_nan</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" href="#models.nn.attention.PointerAttention">PointerAttention</a></code></p>


        <p>Calculate logits given query, key and value and logit key.
This follows the pointer mechanism of Vinyals et al. (2015) <a href="https://arxiv.org/abs/1506.03134">https://arxiv.org/abs/1506.03134</a>,
    and the MoE gating mechanism of Zhou et al. (2024) <a href="https://arxiv.org/abs/2405.01029">https://arxiv.org/abs/2405.01029</a>.</p>


<details class="note" open>
  <summary>Note</summary>
  <p>With Flash Attention, masking is not supported</p>
</details>

<details class="performs-the-following" open>
  <summary>Performs the following</summary>
  <ol>
<li>Apply cross attention to get the heads</li>
<li>Project heads to get glimpse</li>
<li>Compute attention score between glimpse and logit key</li>
</ol>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>total dimension of the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>mask_inner</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to mask inner attention</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>linear_bias</code></b>
          –
          <div class="doc-md-description">
            <p>whether to use bias in linear projection</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>check_nan</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>whether to check for NaNs in logits</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="&lt;code&gt;typing.Callable&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA) implementation</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>moe_kwargs</code></b>
              (<code><a class="autorefs autorefs-external" title="&lt;code&gt;typing.Optional&lt;/code&gt;" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Keyword arguments for MoE</p>
          </div>
        </li>
    </ul>











                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mask_inner</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">out_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_nan</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sdpa_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">moe_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PointerAttnMoE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">mask_inner</span><span class="p">,</span> <span class="n">out_bias</span><span class="p">,</span> <span class="n">check_nan</span><span class="p">,</span> <span class="n">sdpa_fn</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">moe_kwargs</span> <span class="o">=</span> <span class="n">moe_kwargs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_out_moe</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="p">[],</span> <span class="n">out_bias</span><span class="o">=</span><span class="n">out_bias</span><span class="p">,</span> <span class="o">**</span><span class="n">moe_kwargs</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">moe_kwargs</span><span class="p">[</span><span class="s2">&quot;light_version&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_or_moe</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">out_bias</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.MultiHeadCompat" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MultiHeadCompat</span>


<a href="#models.nn.attention.MultiHeadCompat" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MultiHeadCompat</span><span class="p">(</span>
    <span class="n">n_heads</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">val_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">key_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>











<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.MultiHeadCompat.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>:param q: queries (batch_size, n_query, input_dim)</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadCompat</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">val_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># assert embed_dim is not None, &quot;Provide either embed_dim or val_dim&quot;</span>
        <span class="n">val_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="k">if</span> <span class="n">key_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">key_dim</span> <span class="o">=</span> <span class="n">val_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val_dim</span> <span class="o">=</span> <span class="n">val_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">key_dim</span> <span class="o">=</span> <span class="n">key_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">key_dim</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">key_dim</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.attention.MultiHeadCompat.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.attention.MultiHeadCompat.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>:param q: queries (batch_size, n_query, input_dim)
:param h: data (batch_size, graph_size, input_dim)
:param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)
Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)
:return:</p>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    :param q: queries (batch_size, n_query, input_dim)</span>
<span class="sd">    :param h: data (batch_size, graph_size, input_dim)</span>
<span class="sd">    :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)</span>
<span class="sd">    Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">h</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">q</span>  <span class="c1"># compute self-attention</span>

    <span class="c1"># h should be (batch_size, graph_size, input_dim)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">graph_size</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">n_query</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">hflat</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>  <span class="c1">#################   reshape</span>
    <span class="n">qflat</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="c1"># last dimension can be different for keys and values</span>
    <span class="n">shp</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">graph_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">shp_q</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_query</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Calculate queries, (n_heads, n_query, graph_size, key/val_size)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">qflat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shp_q</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hflat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shp</span><span class="p">)</span>

    <span class="c1"># Calculate compatibility (n_heads, batch_size, n_query, graph_size)</span>
    <span class="n">compatibility_s2n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">compatibility_s2n</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.attention.PolyNetAttention" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">PolyNetAttention</span>


<a href="#models.nn.attention.PolyNetAttention" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">PolyNetAttention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">poly_layer_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" href="#models.nn.attention.PointerAttention">PointerAttention</a></code></p>


        <p>Calculate logits given query, key and value and logit key.
This implements a modified version the pointer mechanism of Vinyals et al. (2015) (<a href="https://arxiv.org/abs/1506.03134">https://arxiv.org/abs/1506.03134</a>)
as described in Hottung et al. (2024) (<a href="https://arxiv.org/abs/2402.14048">https://arxiv.org/abs/2402.14048</a>) PolyNetAttention conditions the attention logits on
a set of k different binary vectors allowing to learn k different solution strategies.</p>


<details class="note" open>
  <summary>Note</summary>
  <p>With Flash Attention, masking is not supported</p>
</details>

<details class="performs-the-following" open>
  <summary>Performs the following</summary>
  <ol>
<li>Apply cross attention to get the heads</li>
<li>Project heads to get glimpse</li>
<li>Apply PolyNet layers</li>
<li>Compute attention score between glimpse and logit key</li>
</ol>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>k</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>Number unique bit vectors used to compute attention score</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>embed_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>total dimension of the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>poly_layer_dim</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>Dimension of the PolyNet layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>num_heads</code></b>
              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of heads</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>mask_inner</code></b>
          –
          <div class="doc-md-description">
            <p>whether to mask inner attention</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>linear_bias</code></b>
          –
          <div class="doc-md-description">
            <p>whether to use bias in linear projection</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>check_nan</code></b>
          –
          <div class="doc-md-description">
            <p>whether to check for NaNs in logits</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>sdpa_fn</code></b>
          –
          <div class="doc-md-description">
            <p>scaled dot product attention function (SDPA) implementation</p>
          </div>
        </li>
    </ul>










<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.attention.PolyNetAttention.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Compute attention logits given query, key, value, logit key and attention mask.</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">poly_layer_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PolyNetAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">binary_vector_dim</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">binary_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
            <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binary_vector_dim</span><span class="p">))[:</span><span class="n">k</span><span class="p">]</span>
        <span class="p">),</span>
        <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">poly_layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_vector_dim</span><span class="p">,</span> <span class="n">poly_layer_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">poly_layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">poly_layer_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.attention.PolyNetAttention.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.attention.PolyNetAttention.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">logit_key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute attention logits given query, key, value, logit key and attention mask.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>query</code></b>
          –
          <div class="doc-md-description">
            <p>query tensor of shape [B, ..., L, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>key</code></b>
          –
          <div class="doc-md-description">
            <p>key tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>value</code></b>
          –
          <div class="doc-md-description">
            <p>value tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>logit_key</code></b>
          –
          <div class="doc-md-description">
            <p>logit key tensor of shape [B, ..., S, E]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>attn_mask</code></b>
          –
          <div class="doc-md-description">
            <p>attention mask tensor of shape [B, ..., S]. Note that <code>True</code> means that the value <em>should</em> take part in attention
as described in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">PyTorch Documentation</a></p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">logit_key</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute attention logits given query, key, value, logit key and attention mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        query: query tensor of shape [B, ..., L, E]</span>
<span class="sd">        key: key tensor of shape [B, ..., S, E]</span>
<span class="sd">        value: value tensor of shape [B, ..., S, E]</span>
<span class="sd">        logit_key: logit key tensor of shape [B, ..., S, E]</span>
<span class="sd">        attn_mask: attention mask tensor of shape [B, ..., S]. Note that `True` means that the value _should_ take part in attention</span>
<span class="sd">            as described in the [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute inner multi-head attention with no projections.</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inner_mha</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
    <span class="n">glimpse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="n">num_solutions</span> <span class="o">=</span> <span class="n">glimpse</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_vectors</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_solutions</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">),</span> <span class="mi">1</span><span class="p">)[</span>
        <span class="p">:</span><span class="n">num_solutions</span>
    <span class="p">]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">glimpse</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_solutions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_vector_dim</span><span class="p">)</span>

    <span class="c1"># PolyNet layers</span>
    <span class="n">poly_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poly_layer_1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">glimpse</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">poly_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">poly_out</span><span class="p">)</span>
    <span class="n">poly_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poly_layer_2</span><span class="p">(</span><span class="n">poly_out</span><span class="p">)</span>

    <span class="n">glimpse</span> <span class="o">+=</span> <span class="n">poly_out</span>

    <span class="c1"># Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)</span>
    <span class="c1"># bmm is slightly faster than einsum and matmul</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">glimpse</span><span class="p">,</span> <span class="n">logit_key</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
        <span class="o">-</span><span class="mi">2</span>
    <span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">glimpse</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_nan</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Logits contain NaNs&quot;</span>

    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h2 id="models.nn.attention.scaled_dot_product_attention_simple" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>            <span class="doc doc-object-name doc-function-name">scaled_dot_product_attention_simple</span>


<a href="#models.nn.attention.scaled_dot_product_attention_simple" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">scaled_dot_product_attention_simple</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Simple (exact) Scaled Dot-Product Attention in RL4CO without customized kernels (i.e. no Flash Attention).</p>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/attention.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention_simple</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple (exact) Scaled Dot-Product Attention in RL4CO without customized kernels (i.e. no Flash Attention).&quot;&quot;&quot;</span>

    <span class="c1"># Check for causal and attn_mask conflict</span>
    <span class="k">if</span> <span class="n">is_causal</span> <span class="ow">and</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot set both is_causal and attn_mask&quot;</span><span class="p">)</span>

    <span class="c1"># Calculate scaled dot product</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="c1"># Apply the provided attention mask</span>
    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attn_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">+=</span> <span class="n">attn_mask</span>

    <span class="c1"># Apply causal mask</span>
    <span class="k">if</span> <span class="n">is_causal</span><span class="p">:</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">l_</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">l_</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>

    <span class="c1"># Softmax to get attention weights</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Apply dropout</span>
    <span class="k">if</span> <span class="n">dropout_p</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="c1"># Compute the weighted sum of values</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="multi-layer-perceptron">Multi-Layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="models.nn.mlp.MLP" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">MLP</span>


<a href="#models.nn.mlp.MLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">MLP</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">num_neurons</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#list">list</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
    <span class="n">dropout_probs</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#list">list</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hidden_act</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;ReLU&quot;</span><span class="p">,</span>
    <span class="n">out_act</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;Identity&quot;</span><span class="p">,</span>
    <span class="n">input_norm</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">output_norm</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>












                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/mlp.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
    <span class="n">dropout_probs</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hidden_act</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ReLU&quot;</span><span class="p">,</span>
    <span class="n">out_act</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Identity&quot;</span><span class="p">,</span>
    <span class="n">input_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">output_norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">assert</span> <span class="n">input_norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Batch&quot;</span><span class="p">,</span> <span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="s2">&quot;None&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">output_norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Batch&quot;</span><span class="p">,</span> <span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="s2">&quot;None&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">dropout_probs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dropout_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">dropout_probs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;dropout_probs List length should match the num_neurons List length for MLP, dropouts set to False instead&quot;</span>
        <span class="p">)</span>
        <span class="n">dropout_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_neurons</span> <span class="o">=</span> <span class="n">num_neurons</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_act</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">out_act</span><span class="p">)()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dropout_probs</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">input_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_neurons</span>
    <span class="n">output_dims</span> <span class="o">=</span> <span class="n">num_neurons</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_dim</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">lins</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">output_dims</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_norm_layer</span><span class="p">(</span><span class="n">input_norm</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_norm_layer</span><span class="p">(</span><span class="n">output_norm</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div><h2 id="operations">Operations<a class="headerlink" href="#operations" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="models.nn.ops.PositionalEncoding" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">PositionalEncoding</span>


<a href="#models.nn.ops.PositionalEncoding" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">PositionalEncoding</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_len</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>











<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" href="#models.nn.ops.PositionalEncoding.forward">forward</a></code></b>
            –
            <div class="doc-md-description">
              <p>Arguments:</p>
            </div>
          </li>
    </ul>



                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/ops.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [1, max_len, d_model]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="models.nn.ops.PositionalEncoding.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#models.nn.ops.PositionalEncoding.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">hidden</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">seq_pos</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="&lt;code&gt;torch.Tensor&lt;/code&gt;" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</code></pre></div>

    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>x</code></b>
          –
          <div class="doc-md-description">
            <p>Tensor, shape <code>[batch_size, seq_len, embedding_dim]</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>seq_pos</code></b>
          –
          <div class="doc-md-description">
            <p>Tensor, shape <code>[batch_size, seq_len]</code></p>
          </div>
        </li>
    </ul>


            <details class="quote">
              <summary>Source code in <code>rl4co/models/nn/ops.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">seq_pos</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">        x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``</span>
<span class="sd">        seq_pos: Tensor, shape ``[batch_size, seq_len]``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="n">seq_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span> <span class="o">+</span> <span class="n">pes</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="models.nn.ops.RandomEncoding" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">RandomEncoding</span>


<a href="#models.nn.ops.RandomEncoding" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">RandomEncoding</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span> <span class="n">max_classes</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-external" title="&lt;code&gt;torch.nn.Module&lt;/code&gt;" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code></p>


        <p>This is like torch.nn.Embedding but with rows of embeddings are randomly
permuted in each forward pass before lookup operation. This might be useful
in cases where classes have no fixed meaning but rather indicate a connection
between different elements in a sequence. Reference is the MatNet model.</p>











                  <details class="quote">
                    <summary>Source code in <code>rl4co/models/nn/ops.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_classes</span> <span class="o">=</span> <span class="n">max_classes</span>
    <span class="n">rand_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">max_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;emb&quot;</span><span class="p">,</span> <span class="n">rand_emb</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>












                

              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with ❤️ by <a href="https://github.com/ai4co">AI4CO</a> contributors
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["announce.dismiss", "content.code.copy", "content.code.annotate", "content.code.select", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.preview", "navigation.instant.progress", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.tabs", "search.suggest", "search.highlight", "search.share", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../../../../js/katex.js"></script>
      
        <script src="../../../../js/particles.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>