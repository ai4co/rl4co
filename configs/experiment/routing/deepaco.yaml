# @package _global_

defaults:
  - override /model: deepaco.yaml
  - override /env: tsp.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml

env:
  generator_params:
    num_loc: 50

logger:
  wandb:
    project: "rl4co"
    tags: ["deepaco", "${env.name}"]
    group: ${env.name}${env.generator_params.num_loc}
    name: deepaco-${env.name}${env.generator_params.num_loc}

model:
  batch_size: 20
  val_batch_size: 20
  test_batch_size: 20
  train_data_size: 400
  val_data_size: 20
  test_data_size: 100
  optimizer: "AdamW"
  optimizer_kwargs:
    lr: 5e-4
    weight_decay: 0
  lr_scheduler:
    "CosineAnnealingLR"
  lr_scheduler_kwargs:
    T_max: 50
    eta_min: 1e-5
  metrics:
    test:
      - reward_000
      - reward_002
      - reward_009  # since n_iterations["text"] = 10
  train_with_local_search: True
  ls_reward_aug_W: 0.99

  policy_kwargs:
    n_ants:
      train: 30
      val: 30
      test: 100
    n_iterations:
      train: 1 # unused value
      val: 5
      test: 10
    temperature: 1.0
    top_p: 0.0
    top_k: 0
    start_node: null
    multistart: False
    k_sparse: 5  # this should be adjusted based on the `num_loc` value

    aco_kwargs:
      alpha: 1.0
      beta: 1.0
      decay: 0.95
      use_local_search: True
      use_nls: True
      n_perturbations: 5
      local_search_params:
        max_iterations: 1000
      perturbation_params:
        max_iterations: 20

trainer:
  max_epochs: 50
  gradient_clip_val: 3.0
  precision: "bf16-mixed"
  device:
    - 0

seed: 1234
